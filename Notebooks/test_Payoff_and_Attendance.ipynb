{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-driven Payoff and Attendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import AttendanceM1, AttendanceM2, AttendanceM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "    \"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder_all = Path('../images/Attendance')\n",
    "image_folder_all.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M1 = Path('../images/Attendance/M1')\n",
    "image_folder_M1.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M2 = Path('../images/Attendance/M2')\n",
    "image_folder_M2.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M3 = Path('../images/Attendance/M3')\n",
    "image_folder_M3.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[1] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.16798161486607552 ---- go:0.8320183851339246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.25\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.625\n",
      "Learning rule:\n",
      "Q[1] <- 0.1 + 0.2 * (0.625 - 0.1)\n",
      "Q[1] = 0.20500000000000002\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.205]\n",
      "Action probabilities:\n",
      "no go:0.036263716374648335 ---- go:0.9637362836253517\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.4\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.7\n",
      "Learning rule:\n",
      "Q[1] <- 0.20500000000000002 + 0.2 * (0.7 - 0.20500000000000002)\n",
      "Q[1] = 0.304\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.304]\n",
      "Action probabilities:\n",
      "no go:0.007660409013360771 ---- go:0.9923395909866393\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.5\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.75\n",
      "Learning rule:\n",
      "Q[1] <- 0.304 + 0.2 * (0.75 - 0.304)\n",
      "Q[1] = 0.3932\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.3932]\n",
      "Action probabilities:\n",
      "no go:0.001849110027713592 ---- go:0.9981508899722863\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.5714285714285714\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.7857142857142857\n",
      "Learning rule:\n",
      "Q[1] <- 0.3932 + 0.2 * (0.7857142857142857 - 0.3932)\n",
      "Q[1] = 0.47170285714285715\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.47170286]\n",
      "Action probabilities:\n",
      "no go:0.0005272830080689828 ---- go:0.999472716991931\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.625\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8125\n",
      "Learning rule:\n",
      "Q[1] <- 0.47170285714285715 + 0.2 * (0.8125 - 0.47170285714285715)\n",
      "Q[1] = 0.5398622857142857\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.53986229]\n",
      "Action probabilities:\n",
      "no go:0.0001772456680178412 ---- go:0.9998227543319822\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.6666666666666666\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8333333333333333\n",
      "Learning rule:\n",
      "Q[1] <- 0.5398622857142857 + 0.2 * (0.8333333333333333 - 0.5398622857142857)\n",
      "Q[1] = 0.5985564952380953\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.5985565]\n",
      "Action probabilities:\n",
      "no go:6.930640494320914e-05 ---- go:0.9999306935950568\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.7\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.85\n",
      "Learning rule:\n",
      "Q[1] <- 0.5985564952380953 + 0.2 * (0.85 - 0.5985564952380953)\n",
      "Q[1] = 0.6488451961904762\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 1.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (1, 1) is: 0.0\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[1] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 1.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[1] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 1.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[0] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0.1 0. ]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339246 ---- go:0.16798161486607552\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.75\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.375\n",
      "Learning rule:\n",
      "Q[0] <- 0.1 + 0.2 * (0.375 - 0.1)\n",
      "Q[0] = 0.15500000000000003\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0.155 0.   ]\n",
      "Action probabilities:\n",
      "no go:0.9227277978633401 ---- go:0.07727220213665986\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.6\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.2\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.2 - 0.0)\n",
      "Q[1] = -0.04000000000000001\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [ 0.155 -0.04 ]\n",
      "Action probabilities:\n",
      "no go:0.9577102281579661 ---- go:0.04228977184203378\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.6666666666666666\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.3333333333333333\n",
      "Learning rule:\n",
      "Q[0] <- 0.15500000000000003 + 0.2 * (0.3333333333333333 - 0.15500000000000003)\n",
      "Q[0] = 0.19066666666666668\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.19066667 -0.04      ]\n",
      "Action probabilities:\n",
      "no go:0.9756522473339502 ---- go:0.02434775266604975\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5714285714285714\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.2857142857142857\n",
      "Learning rule:\n",
      "Q[0] <- 0.19066666666666668 + 0.2 * (0.2857142857142857 - 0.19066666666666668)\n",
      "Q[0] = 0.2096761904761905\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.20967619 -0.04      ]\n",
      "Action probabilities:\n",
      "no go:0.9819220515055397 ---- go:0.018077948494460344\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.25\n",
      "Learning rule:\n",
      "Q[0] <- 0.2096761904761905 + 0.2 * (0.25 - 0.2096761904761905)\n",
      "Q[0] = 0.2177409523809524\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.21774095 -0.04      ]\n",
      "Action probabilities:\n",
      "no go:0.9840756789629249 ---- go:0.015924321037075098\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4444444444444444\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.2222222222222222\n",
      "Learning rule:\n",
      "Q[0] <- 0.2177409523809524 + 0.2 * (0.2222222222222222 - 0.2177409523809524)\n",
      "Q[0] = 0.21863720634920636\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.21863721 -0.04      ]\n",
      "Action probabilities:\n",
      "no go:0.9842988453539557 ---- go:0.015701154646044265\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.2\n",
      "Learning rule:\n",
      "Q[0] <- 0.21863720634920636 + 0.2 * (0.2 - 0.21863720634920636)\n",
      "Q[0] = 0.2149097650793651\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[1] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.16798161486607552 ---- go:0.8320183851339246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.33333333333333337\n",
      "Learning rule:\n",
      "Q[1] <- 0.1 + 0.2 * (-0.33333333333333337 - 0.1)\n",
      "Q[1] = 0.013333333333333336\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.01333333]\n",
      "Action probabilities:\n",
      "no go:0.4468680219310488 ---- go:0.5531319780689513\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.5\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 1] is: 0.75\n",
      "Learning rule:\n",
      "Q[1] <- 0.013333333333333336 + 0.2 * (0.75 - 0.013333333333333336)\n",
      "Q[1] = 0.16066666666666668\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.16066667]\n",
      "Action probabilities:\n",
      "no go:0.07105029006848465 ---- go:0.9289497099315153\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.6\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.2\n",
      "Learning rule:\n",
      "Q[1] <- 0.16066666666666668 + 0.2 * (-0.2 - 0.16066666666666668)\n",
      "Q[1] = 0.08853333333333334\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.08853333]\n",
      "Action probabilities:\n",
      "no go:0.19520562288094168 ---- go:0.8047943771190583\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.6666666666666666\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 1] is: 0.8333333333333333\n",
      "Learning rule:\n",
      "Q[1] <- 0.08853333333333334 + 0.2 * (0.8333333333333333 - 0.08853333333333334)\n",
      "Q[1] = 0.23749333333333333\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.23749333]\n",
      "Action probabilities:\n",
      "no go:0.02188355398386017 ---- go:0.9781164460161399\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.7142857142857143\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.14285714285714285\n",
      "Learning rule:\n",
      "Q[1] <- 0.23749333333333333 + 0.2 * (-0.14285714285714285 - 0.23749333333333333)\n",
      "Q[1] = 0.1614232380952381\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.16142324]\n",
      "Action probabilities:\n",
      "no go:0.07025546150915718 ---- go:0.9297445384908428\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.75\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 1] is: 0.875\n",
      "Learning rule:\n",
      "Q[1] <- 0.1614232380952381 + 0.2 * (0.875 - 0.1614232380952381)\n",
      "Q[1] = 0.3041385904761905\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.30413859]\n",
      "Action probabilities:\n",
      "no go:0.00764357097121514 ---- go:0.9923564290287848\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.7777777777777778\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.1111111111111111\n",
      "Learning rule:\n",
      "Q[1] <- 0.3041385904761905 + 0.2 * (-0.1111111111111111 - 0.3041385904761905)\n",
      "Q[1] = 0.2210886501587302\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.22108865]\n",
      "Action probabilities:\n",
      "no go:0.02826611077395729 ---- go:0.9717338892260428\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.8\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.4\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.4 - 0.0)\n",
      "Q[0] = 0.08000000000000002\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61773a0e72824a3e8dd6958e202a0a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d01d73977364ebf9ca3396e397352b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d126a7d29404f6e83ccfb3026366ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b79c97160124b919b58b2dbceec6db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3090f811ed34238a0f04f90addb1dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M1\\conditional_entropy_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M1\\entropy_bias.png\n",
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Attendance\\M1\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Attendance\\M1\\inequality_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AttendanceM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.5\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 0.75\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (0.75 - 0.0)\n",
      "Q[(0, 0), 1] = 0.15000000000000002\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.6666666666666666\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8333333333333333\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (0.8333333333333333 - 0.0)\n",
      "Q[(1, 0), 1] = 0.16666666666666666\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.16666667]\n",
      "Action probabilities:\n",
      "no go:0.06496916912866407 ---- go:0.935030830871336\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.75\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.875\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.16666666666666666 + 0.2 * (0.875 - 0.16666666666666666)\n",
      "Q[(1, 0), 1] = 0.30833333333333335\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.30833333]\n",
      "Action probabilities:\n",
      "no go:0.007150950639303529 ---- go:0.9928490493606965\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.8\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.30833333333333335 + 0.2 * (0.9 - 0.30833333333333335)\n",
      "Q[(1, 0), 1] = 0.4266666666666667\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.42666667]\n",
      "Action probabilities:\n",
      "no go:0.0010832921948149817 ---- go:0.998916707805185\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.8333333333333334\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9166666666666667\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.4266666666666667 + 0.2 * (0.9166666666666667 - 0.4266666666666667)\n",
      "Q[(1, 0), 1] = 0.5246666666666667\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.52466667]\n",
      "Action probabilities:\n",
      "no go:0.00022601872436822605 ---- go:0.9997739812756318\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.8571428571428571\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9285714285714286\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.5246666666666667 + 0.2 * (0.9285714285714286 - 0.5246666666666667)\n",
      "Q[(1, 0), 1] = 0.6054476190476191\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.60544762]\n",
      "Action probabilities:\n",
      "no go:6.207147724466373e-05 ---- go:0.9999379285227552\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.875\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9375\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.6054476190476191 + 0.2 * (0.9375 - 0.6054476190476191)\n",
      "Q[(1, 0), 1] = 0.6718580952380953\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.6718581]\n",
      "Action probabilities:\n",
      "no go:2.1450749428927884e-05 ---- go:0.9999785492505711\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.8888888888888888\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9444444444444444\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.6718580952380953 + 0.2 * (0.9444444444444444 - 0.6718580952380953)\n",
      "Q[(1, 0), 1] = 0.7263753650793651\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.72637537]\n",
      "Action probabilities:\n",
      "no go:8.966503046322675e-06 ---- go:0.9999910334969536\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.9\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.95\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7263753650793651 + 0.2 * (0.95 - 0.7263753650793651)\n",
      "Q[(1, 0), 1] = 0.7711002920634921\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.77110029]\n",
      "Action probabilities:\n",
      "no go:4.383736755526708e-06 ---- go:0.9999956162632445\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.9090909090909091\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.9545454545454546\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7711002920634921 + 0.2 * (0.9545454545454546 - 0.7711002920634921)\n",
      "Q[(1, 0), 1] = 0.8077893245598846\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = AttendanceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (1, 1) is: 0.25\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.25 - 0.0)\n",
      "Q[(1, 1), 0] = 0.05\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.16666666666666666\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0.16666666666666666 - 0.0)\n",
      "Q[(0, 1), 0] = 0.03333333333333333\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0.03333333 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.6302602229177513 ---- go:0.3697397770822487\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.25\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.125\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.03333333333333333 + 0.2 * (0.125 - 0.03333333333333333)\n",
      "Q[(0, 1), 0] = 0.051666666666666666\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0.05166667 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.6956496530565213 ---- go:0.3043503469434787\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.2\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.1\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.051666666666666666 + 0.2 * (0.1 - 0.051666666666666666)\n",
      "Q[(0, 1), 0] = 0.06133333333333334\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0.06133333 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.7273726986912211 ---- go:0.27262730130877894\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.16666666666666666\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.08333333333333333\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.06133333333333334 + 0.2 * (0.08333333333333333 - 0.06133333333333334)\n",
      "Q[(0, 1), 0] = 0.06573333333333334\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.06573333 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.7411076078215805 ---- go:0.25889239217841953\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.14285714285714285\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.07142857142857142\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.06573333333333334 + 0.2 * (0.07142857142857142 - 0.06573333333333334)\n",
      "Q[(0, 1), 0] = 0.06687238095238096\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [0.06687238 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.7445889466187986 ---- go:0.25541105338120146\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.125\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.0625\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.06687238095238096 + 0.2 * (0.0625 - 0.06687238095238096)\n",
      "Q[(0, 1), 0] = 0.06599790476190477\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.0659979 0.       ]\n",
      "Action probabilities:\n",
      "no go:0.7419189794239515 ---- go:0.2580810205760486\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.2222222222222222\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.3888888888888889\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (-0.3888888888888889 - 0.0)\n",
      "Q[(0, 1), 1] = -0.07777777777777778\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 1]: [0.05 0.  ]\n",
      "Action probabilities:\n",
      "no go:0.6899744811276124 ---- go:0.31002551887238755\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.3\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 1] is: -0.35\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (-0.35 - 0.0)\n",
      "Q[(1, 1), 1] = -0.06999999999999999\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [ 0.05 -0.07]\n",
      "Action probabilities:\n",
      "no go:0.8721384336809186 ---- go:0.12786156631908133\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.2727272727272727\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.13636363636363635\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.05 + 0.2 * (0.13636363636363635 - 0.05)\n",
      "Q[(1, 1), 0] = 0.06727272727272728\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.0\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[(0, 0), 0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.6666666666666666\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (0.6666666666666666 - 0.0)\n",
      "Q[(0, 1), 1] = 0.13333333333333333\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.5\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.25\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (-0.25 - 0.0)\n",
      "Q[(1, 0), 1] = -0.05\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.2\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.2 - 0.0)\n",
      "Q[(1, 1), 0] = 0.04000000000000001\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.16666666666666666\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (0.16666666666666666 - 0.0)\n",
      "Q[(0, 0), 0] = 0.03333333333333333\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.13333333]\n",
      "Action probabilities:\n",
      "no go:0.10589896223591787 ---- go:0.8941010377640822\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.42857142857142855\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7142857142857143\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.13333333333333333 + 0.2 * (0.7142857142857143 - 0.13333333333333333)\n",
      "Q[(0, 1), 1] = 0.24952380952380954\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [ 0.   -0.05]\n",
      "Action probabilities:\n",
      "no go:0.6899744811276125 ---- go:0.31002551887238755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.375\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.1875\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0.1875 - 0.0)\n",
      "Q[(1, 0), 0] = 0.037500000000000006\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.24952381]\n",
      "Action probabilities:\n",
      "no go:0.01812127834516898 ---- go:0.981878721654831\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.4444444444444444\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7222222222222222\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.24952380952380954 + 0.2 * (0.7222222222222222 - 0.24952380952380954)\n",
      "Q[(0, 1), 1] = 0.34406349206349207\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [ 0.0375 -0.05  ]\n",
      "Action probabilities:\n",
      "no go:0.8021838885585818 ---- go:0.19781611144141825\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.2\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.037500000000000006 + 0.2 * (0.2 - 0.037500000000000006)\n",
      "Q[(1, 0), 0] = 0.07\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.34406349]\n",
      "Action probabilities:\n",
      "no go:0.00404985600871304 ---- go:0.995950143991287\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.45454545454545453\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7272727272727273\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.34406349206349207 + 0.2 * (0.7272727272727273 - 0.34406349206349207)\n",
      "Q[(0, 1), 1] = 0.42070533910533914\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f825b526ea4fc0acd153fc49dfc569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07825f2386c45838c9cba36944ac6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9012531c11824d2d84d59254d7463c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f634355bbb4692aa2433dad97a38b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b151f628bf044f49b41389b52745197c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Attendance\\M2\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Attendance\\M2\\inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M2\\entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M2\\conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AttendanceM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.0\n",
      "Learning rule:\n",
      "Q[(0, 0),0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[(0, 0),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.14285714285714285\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.5714285714285714\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.0 + 0.2 * (0.5714285714285714 - 0.0)\n",
      "Q[[0, 0],1] = 0.11428571428571428\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.125\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.0625\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0.0625 - 0.0)\n",
      "Q[[1, 0],0] = 0.0125\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.         0.11428571]\n",
      "Action probabilities:\n",
      "no go:0.13840854464454283 ---- go:0.8615914553554572\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.2222222222222222\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.6111111111111112\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.11428571428571428 + 0.2 * (0.6111111111111112 - 0.11428571428571428)\n",
      "Q[[0, 0],1] = 0.21365079365079365\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.0125 0.    ]\n",
      "Action probabilities:\n",
      "no go:0.5498339973124778 ---- go:0.4501660026875221\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.2\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.1\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0125 + 0.2 * (0.1 - 0.0125)\n",
      "Q[[1, 0],0] = 0.030000000000000002\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.         0.21365079]\n",
      "Action probabilities:\n",
      "no go:0.0317249516437436 ---- go:0.9682750483562564\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.2727272727272727\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.6363636363636364\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.21365079365079365 + 0.2 * (0.6363636363636364 - 0.21365079365079365)\n",
      "Q[[0, 0],1] = 0.2981933621933622\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = AttendanceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (1, 1) is: 0.25\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.2 * (0.25 - 0.0)\n",
      "Q[(1, 1),0] = 0.05\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.6666666666666666\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.16666666666666669\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (-0.16666666666666669 - 0.0)\n",
      "Q[[0, 1],1] = -0.03333333333333334\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0.05 0.  ]\n",
      "Action probabilities:\n",
      "no go:0.6899744811276124 ---- go:0.31002551887238755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.25\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.05 + 0.2 * (0.25 - 0.05)\n",
      "Q[[1, 1],0] = 0.09000000000000001\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.03333333]\n",
      "Action probabilities:\n",
      "no go:0.6302602229177513 ---- go:0.36973977708224864\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.6\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.2\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.03333333333333334 + 0.2 * (-0.2 - -0.03333333333333334)\n",
      "Q[[0, 1],1] = -0.06666666666666668\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [0.09 0.  ]\n",
      "Action probabilities:\n",
      "no go:0.8084546514385325 ---- go:0.1915453485614675\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.25\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.09000000000000001 + 0.2 * (0.25 - 0.09000000000000001)\n",
      "Q[[1, 1],0] = 0.122\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.06666667]\n",
      "Action probabilities:\n",
      "no go:0.7439624913247581 ---- go:0.25603750867524194\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.42857142857142855\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.21428571428571427\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0.21428571428571427 - 0.0)\n",
      "Q[[0, 1],0] = 0.04285714285714286\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.04285714 -0.06666667]\n",
      "Action probabilities:\n",
      "no go:0.852252858063711 ---- go:0.14774714193628896\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.5\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.25\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.06666666666666668 + 0.2 * (-0.25 - -0.06666666666666668)\n",
      "Q[[0, 1],1] = -0.10333333333333335\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.122 0.   ]\n",
      "Action probabilities:\n",
      "no go:0.875664557746637 ---- go:0.12433544225336304\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4444444444444444\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.2222222222222222\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.122 + 0.2 * (0.2222222222222222 - 0.122)\n",
      "Q[[1, 1],0] = 0.14204444444444445\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.04285714 -0.10333333]\n",
      "Action probabilities:\n",
      "no go:0.9120597277317503 ---- go:0.08794027226824981\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.2\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.04285714285714286 + 0.2 * (0.2 - 0.04285714285714286)\n",
      "Q[[0, 1],0] = 0.07428571428571429\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.07428571 -0.10333333]\n",
      "Action probabilities:\n",
      "no go:0.9448987179765859 ---- go:0.05510128202341415\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.36363636363636365\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.18181818181818182\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.07428571428571429 + 0.2 * (0.18181818181818182 - 0.07428571428571429)\n",
      "Q[[0, 1],0] = 0.09579220779220779\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.0\n",
      "Learning rule:\n",
      "Q[(0, 0),0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[(0, 0),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.6666666666666666\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (0.6666666666666666 - 0.0)\n",
      "Q[[0, 1],1] = 0.13333333333333333\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go: 0.5\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.25\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (-0.25 - 0.0)\n",
      "Q[[1, 0],1] = -0.05\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.2\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0.2 - 0.0)\n",
      "Q[[1, 1],0] = 0.04000000000000001\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.3333333333333333\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.16666666666666666\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.16666666666666666 - 0.0)\n",
      "Q[[0, 0],0] = 0.03333333333333333\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.13333333]\n",
      "Action probabilities:\n",
      "no go:0.10589896223591787 ---- go:0.8941010377640822\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.42857142857142855\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7142857142857143\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.13333333333333333 + 0.2 * (0.7142857142857143 - 0.13333333333333333)\n",
      "Q[[0, 1],1] = 0.24952380952380954\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [ 0.   -0.05]\n",
      "Action probabilities:\n",
      "no go:0.6899744811276125 ---- go:0.31002551887238755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.375\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.1875\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0.1875 - 0.0)\n",
      "Q[[1, 0],0] = 0.037500000000000006\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.24952381]\n",
      "Action probabilities:\n",
      "no go:0.01812127834516898 ---- go:0.981878721654831\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.4444444444444444\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7222222222222222\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.24952380952380954 + 0.2 * (0.7222222222222222 - 0.24952380952380954)\n",
      "Q[[0, 1],1] = 0.34406349206349207\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [ 0.0375 -0.05  ]\n",
      "Action probabilities:\n",
      "no go:0.8021838885585818 ---- go:0.19781611144141825\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go: 0.4\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.2\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.037500000000000006 + 0.2 * (0.2 - 0.037500000000000006)\n",
      "Q[[1, 0],0] = 0.07\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.34406349]\n",
      "Action probabilities:\n",
      "no go:0.00404985600871304 ---- go:0.995950143991287\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go: 0.45454545454545453\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.7272727272727273\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.34406349206349207 + 0.2 * (0.7272727272727273 - 0.34406349206349207)\n",
      "Q[[0, 1],1] = 0.42070533910533914\n"
     ]
    }
   ],
   "source": [
    "agent = AttendanceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a183c5a457a04239a91d5ef16582069a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190ff531e7c24585a13ceb05c6bf4ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7eb4a138a042c68b975690a6b659b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ab19b41d71453e99fdcdf69b77af0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2016852aacdc4521a7d25b0a83475918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Attendance\\M3\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Attendance\\M3\\inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M3\\entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Attendance\\M3\\conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AttendanceM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': AttendanceM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': AttendanceM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': AttendanceM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5e91bceffb48e9a93b7d737fb6b543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4793a332921a47899953753db82ec9c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ee239ba0cf4703a99081a1f04aa85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Attendance\\efficiency.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Attendance\\inequality.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Attendance\\entropy.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Attendance\\conditional_entropy.png\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder_all,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Attendance-M1-7': 'M1',\n",
    "            'Attendance-M2-9': 'M2',\n",
    "            'Attendance-M3-0': 'M3'\n",
    "        },\n",
    "        'figsize': (3.5, 3)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':1,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [12, 12, 40, 46]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6ed9d858de4f1481b5f86b843bced0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff5b8481e9b45c3b96209ff764df4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74b9ac4a653440490a2d482df892c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10bf417f5314442a2a9822389e2a68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51939911f51041c9bbd8c0a77b43b9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=AttendanceM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [6, 32, 82, 84]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cade1546c8c4fbdb385e29cb8e9787b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d930ba90be944c84af75f3dc9d13b15b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade09956cea84a82b94de593a1cfa40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afad0c0bbdb646b5b0ca16974db8a395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dc4da1b9114cab996f0a8b6db26980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=AttendanceM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [49, 55, 73, 60]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811f2e32edd447dfbbf0943347782c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677b4b0be6864e3db9befed3fbe4805f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faeb8c9d78544801be84ccfd7c051f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d265a684bf6641168dcd4feb4f932820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb38e1c6a914a65a8bdc412f5fba1bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=AttendanceM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
