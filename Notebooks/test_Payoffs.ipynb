{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-driven Payoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import PayoffM1, PayoffM2, PayoffM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder_M1 = Path('../images/Payoff/M1')\n",
    "image_folder_M1.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M2 = Path('../images/Payoff/M2')\n",
    "image_folder_M2.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M3 = Path('../images/Payoff/M3')\n",
    "image_folder_M3.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[1] = 0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[1] = 0.488\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[1] = 0.5904\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[1] = 0.67232\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[1] = 0.7378560000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[1] = 0.7902848\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[1] = 0.83222784\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[1] = 0.865782272\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (1, 1) is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[1] = 0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[1] = 0.488\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[1] = 0.5904\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[1] = 0.67232\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 1]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[1] = 0.7378560000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[1] = 0.7902848\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 1]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[1] = 0.83222784\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[1] = 0.865782272\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[1] = 0.2\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[1] = 0.36000000000000004\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[1] = 0.488\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[1] = 0.5904\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[1] = 0.67232\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[1] = 0.7378560000000001\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[1] = 0.7902848\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[1] = 0.83222784\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b092757759412f895c2833c00cc7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d70ef305164eb8abec3d6a9847d8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832844f1152045e7851ae3079a7187bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9145f6a445474064ba51b3e562ee5a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9690feb8be5f46f99503634c65d54d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d98098461374ba8acd98207f45d253d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cab6602dfd24148ba192fa0c6585d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Payoff/M1/conditional_entropy_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M1/entropy_learning_rate.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M1/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Payoff/M1/inequality_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae46c2b3eb044cf9fddd12fb250dc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9d1e9cf471c464291b11146627993d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07247c80cac842d79c12d11c8e47944e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f34aa2bf1884fd0a3d9edc2dc2f07b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f6fb84440842e1ad62d976ebf38056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11e27a67c14414ab147e9c6a4ddc71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21c7f965e634bbebebcde755a5a7d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5074b062f71440dc8cf40f430c0442cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M1/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M1/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX_string = Performer.simple_plots(\n",
    "\tagent_class=QAttendance,\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tsimulation_parameters=simulation_parameters,\n",
    "\tmeasures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "\timage_folder=image_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0e8485b00d419cbe38ab060d85acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3f3d37cb9d412a8709aa985d003c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1d433713c84c35bdc1b0dfacc70143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1499fcc2604f89a24c263d5953000d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "673b3b3056404d96a222e2cf4d304c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b7845d2272414b9c5818461ed48122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0b5238ebee4a2f958c50ebd2e3a227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a33b57b7233443da2a8ba7605dafc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6796b03607a4ff9accc4a9d6bf8b95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101258abb9ef4371934997cd51d21fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18931c5ba48a4055b1fa98d1f8b34cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83400fd184c84ed2bd970edf825fb127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3146b0069197435bb55549580f1eeac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e4953a90ed43a3888cb31225c698b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4e8e15df4c447ba72cc51ba212ef4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ec98a32ab14b2cb6afcb78ed28f8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b62b4e134cf49c1941e562f34131f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363be26250a4403ca596b403c857552c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57190d5dc29e4158872898a8f362bb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7906575c9e75459f8464d01b20ed5a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d78be1f15448ffbe8ef59c98e34223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b96a659aee499a99daa54e4afc2dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2fd976ddc1469e9bad44b218efbae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4cbde975c747de943006069547ac6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc46e29b90c41e6b29e580784a3fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83eb9dcc926246c6a0c6583f1dd38ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939dbaa49e574c379bf4d841536b2f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014f5343864407eb87951f13f212a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9c9b3aa83d482c94fe65388c1fe934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861fa767947f46a7af1a73f39841d5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c91ec228e104bbb9ce55c397193e3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0897bf8ddf8a410c9c35564222ec7b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc52de6c904401abc1eb76e91d7e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeeef9b96bb4dbca12d537cc3683491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dadbdd77950484e83890c6b5a04c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab448a750f54dd1b804426c78341616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407d4428864942ff9cd9a8492a948eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3481928a6f4f5b8c6d4535c55cd9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e5b89e030e4c67965da5159d191ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac90b3dfe884be8968452d569ccaa09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0bafd4d7d44a73b1e7cb97db3594ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8717cf6af7e74fa3a870653069437042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192b0b85caac46cf84859a75c372ff53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f97dfd5b0e4a158da9c3ae1011cb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07465d3973a4488b8d02f2376d1524ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa06225d49b4210b278384803a21587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7260b88549e14b79a41afdb94c96840b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be29ef41fd3450dbf03d759f53adf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa00b116be17481bab150f837f175462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8f0a001d1a4ed3a543ff6fcccd819d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder_M1 / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M1/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder_M1 / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M1/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder_M1 / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (0, 0) is: 1\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(0, 0), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 0), 1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 0), 1] = 0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[(1, 0), 1] = 0.488\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[(1, 0), 1] = 0.5904\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[(1, 0), 1] = 0.67232\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[(1, 0), 1] = 0.7378560000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[(1, 0), 1] = 0.7902848\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[(1, 0), 1] = 0.83222784\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[(1, 0), 1] = 0.865782272\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (1, 1) is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 1), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 1), 1] = 0.36000000000000004\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[(1, 1), 1] = 0.488\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[(1, 1), 1] = 0.5904\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[(1, 1), 1] = 0.67232\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[(1, 1), 1] = 0.7378560000000001\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 1]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[(1, 1), 1] = 0.7902848\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[(1, 1), 1] = 0.83222784\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[(1, 1), 1] = 0.865782272\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [0.         0.86578227]\n",
      "Action probabilities:\n",
      "no go:9.636696231312355e-07 ---- go:0.9999990363303768\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.865782272 + 0.2 * (1 - 0.865782272)\n",
      "Q[(1, 1), 1] = 0.8926258176\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (0, 0) is: 1\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(0, 0), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 1), 1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 0), 1] = 0.2\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 1), 1] = 0.36000000000000004\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 0), 1] = 0.36000000000000004\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[(1, 1), 1] = 0.488\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[(1, 0), 1] = 0.488\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[(1, 1), 1] = 0.5904\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[(1, 0), 1] = 0.5904\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[(1, 1), 1] = 0.67232\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835fe5eb57cc41dfba16f5845e6ddfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644f881f63de40bba461ce592f882032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e76a26d9b24733a70566c2709beeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0280809c7958447fb6cee85f8e44c4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b36479006c4510ab54540a3c7b8c0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d3f37c7c6249d7823027950e6c4715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ce43716c074b3da180e42d3fc51ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M2/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Payoff/M2/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M2/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Payoff/M2/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6b6e4e5d164b5cbfeb9a0a3eaad3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c2b525dec4461b96420057919fe6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74bd91eb9e6414794c247a06d65add6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ddcb558cbf4e208adbb77714407ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956e1be19ef5493b87f98b16165b28af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88972b50fe484a7f8a8662a856b26ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610f40f0fb3440f392f894a0aef647cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2093a3347d4c41838c9469b255becb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M2/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M2/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca106d09b9144eb9fd90f0a34af6b93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003fe2ac8c95411c9bd8b0440f96446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596ac00d6b9e4a6980017f06f9f49d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7f5887242942cdad1958b884344a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f72d7e3d7ac4c039d47991336cd0914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754a6fe174b94dfcb3ac5269cee2303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49530d125454b5e92a8d6250c13f540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1fdf4345664eb8ad864405371d5eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0a52707af240f390dfb2b441c23c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce315023e2f94d16a609268c18eed3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf41668a6726444d94f5985489168d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f232e98d04992884f3dcec4824b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e1ffff35fc4bcbbfb28a348ea8e051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df91350100184d13b6e855a22f92d870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f1eb3095484ddd92eca40cc66723fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f791791f6e41df91321285bd8e67bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48587d1168e4c6da96e4097aee637b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163a1a7e9df84841af04db1e94b9ace6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a64f351006d4e8f9f4c1a180ca46070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8f43acfee44f98937f9eb836fe9ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df04588bb99f455fbbf0593da4654bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9370265966d24fccb678f9daaadf72d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "601d95ee9c854838aef6c7a98ada36fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1043a22c050249f29d9d20807fcb0dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425a2f6784184461bf9193f86fa58440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d56fc9e4954ee993659c8e62739d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df6696195c94abe872ae67ffbaf8f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd65d61963c4cb1b21d72425b3dfeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd800ae518544527acd7e6239af5784c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d44b9ca72124fb1b4b94df0c1f6edac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e43d7d092a740419588349d06cad010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df12986b77f4fc79c12fd4379f7facc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364e5616075741d591b8c4b122d8f17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4325331007974a95bf29f37fcefcac6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41ea170fc5194835b6906c33efdfcff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da35785e40e944f29a34af7968a16e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912254de065747f0a5d9bf2ad77f4c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33058a95b22f498f8ffaf5f08cd423f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae93180950e46aea642f8bb99941b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4d7aadfb74486bb0e6d6573fa085e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ef3721100043d18b2bbabf8bd12ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ddb82104b5434d9bfe8821036c7311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16108ef16eca422ca3432bbcaddf9b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0062257150954a4389712df4a663868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b8a9558d864a32aaeb5a0f5e60feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd34273860a47d88ad7d28bb0b42aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9208de9fc05f473d9a11193219cfefbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fb3d4c030f4c40902f39ff16b440fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc5631bee304c6ba893391f4970a83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baebc4c2a6a44eecb5a74e6920c6c928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder_M2 / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M2/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder_M2 / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M2/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder_M2 / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (0, 0) is: 1\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(0, 0),1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[1, 0],1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[[1, 0],1] = 0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[[1, 0],1] = 0.488\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[[1, 0],1] = 0.5904\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[[1, 0],1] = 0.67232\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[[1, 0],1] = 0.7378560000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[[1, 0],1] = 0.7902848\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[[1, 0],1] = 0.83222784\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[[1, 0],1] = 0.865782272\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (1, 1) is: 0\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 1),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[[0, 1],1] = -0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 1],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.2 + 0.2 * (-1 - -0.2)\n",
      "Q[[0, 1],1] = -0.36000000000000004\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 1],0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (0, 0) is: -1\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[(0, 0),1] = -0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 1],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[1, 1],1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 0],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[0, 1],1] = 0.2\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 0],0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[[0, 1],1] = 0.36000000000000004\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 0],0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[[0, 1],1] = 0.488\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117aa68c540444869805a5406cc318e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af656d81243342a0bc11a7706b56e268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd7d82450204cec9e22c19e5a27924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a21e5d3d03448d388db2f17e154078f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5797204b91ed4ab0a65986b28ab5d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fff440f8c0405a9153780ebee2f7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fed00b0deb49d6944d19fa37cfbd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M3/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Payoff/M3/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M3/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Payoff/M3/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5777618829cb4c5b94ef4a57aae24b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be84f5e550874ae0ba02bf0dc771f1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090e7957726a439eac42380e842f0525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c174c47b13b40e0985e13ded3f5b793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c15c6149b24c7692105b5db40bfff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef69f74c983340c7be21e33ce270d2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eef913ddcc2437fa6b87d4ef3e17b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11751ba4e474b7d808b88240ecd607c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M3/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M3/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c6f68a018b4ecdb8a6ca92c619e0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2addab2c194f0e99d4082dcf72b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ce42b18534900aaebc1fd3a4095a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5669fbc6bc7147d1ada675c70ffca5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "825524d84d4a4870a55b6d27be7a400b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aaa636634542c9b931661be6fc2bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61ea8fd89484a38be1f6fbe42df53ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ec3b6abe1f421b8e80604e10a5800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29aa1d55c298406794dadc50769e19f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2f67abc1e04e1fa37ea7eadbefb4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccce815df7b64677aee5fed138edee1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a193f6189b145a6a9b8c730332ccf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a4fe45d86e419bac20751f18e6af9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4759583798f2472c82a0dfe2cc8863db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18834100a874776b75bd22a3db84fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e8af9ccc634318a8de3319320c5608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741d1eaebde54a3b9511c4325e411ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b225891e62e84505bfab0b57e4456913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c4f13a5a7b4916bf9d7f1d3d93dcdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e358ddadec734f42aeeee20b139beb23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3f18de02364bc694d2b8ab56f6c07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2514cc92034f9593bc3128dc433982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9caf990e4b604d3381e022336bfa049b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39561fd078b4353bf8b268e538b4099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d657f4dc97c48f5bc5ae36541a16f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f8f54f17f149018a27ac96924cefcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a62d737b5ed4cdb9ce50638f718c025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce889255a4c4780b3951e095d1d3974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655f6a174afb49c3b6887088a23a15e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b78b0e7797444c88ffc99b213b1d2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bb3683a73d4bf6a35a9f11189fb21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e0d36a27c64c93ba4e067a96f2444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4795249aa48b40af96e8165946ffe089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3071d4f8de4a4f6782d023add1391270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ad934b3f9846e2a93ce166847bce37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ead780fbd8467cbeba7c3cb55f1322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e38168c1ed43c79f3df4a049e70ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2756d9f283f34d0cae1114cee4aeefa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67de5d3b458449f2ad0fe714d0919383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b938b5e57aee4833ac00a45b821e6fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd767a23edf04c96b6ae4fb2975d6222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3685719975e4f86b6fb654d685fd108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4aeacb1b9547df86420ebab4dae670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38e57969d4d4451ae0b9460f46b644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1db387e49ff44469058ecb302d7a19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e89e20c3c284d65ba4c9c4397831e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173c49e03e0a4febac6b052e40bb7b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9796916711fb4c309151ced37a21717c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c1d69d44584620a6e20367bad2dc9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "727fe3de0ebe4dce8fb995a4b96aea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M3/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/Payoff/M3/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/Payoff')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/Payoff')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': PayoffM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': PayoffM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': PayoffM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Payoff-M1-7': 'M1',\n",
    "            'Payoff-M2-9': 'M2',\n",
    "            'Payoff-M3-0': 'M3'\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t'inverse_temperature':16,\n",
    "\t'learning_rate':1.2\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':1,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../images/Payoff/M1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m image_folder\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m LaTeX_string \u001b[38;5;241m=\u001b[39m Performer\u001b[38;5;241m.\u001b[39msimple_run(\n\u001b[1;32m      5\u001b[0m     agent_class\u001b[38;5;241m=\u001b[39mPayoffM1,\n\u001b[1;32m      6\u001b[0m     fixed_parameters\u001b[38;5;241m=\u001b[39mfixed_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     measures\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "image_folder = Path('../images/Payoff/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../images/Payoff/M2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m image_folder\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m LaTeX_string \u001b[38;5;241m=\u001b[39m Performer\u001b[38;5;241m.\u001b[39msimple_run(\n\u001b[1;32m      5\u001b[0m     agent_class\u001b[38;5;241m=\u001b[39mPayoffM2,\n\u001b[1;32m      6\u001b[0m     fixed_parameters\u001b[38;5;241m=\u001b[39mfixed_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     measures\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "image_folder = Path('../images/Payoff/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [91, 87, 79, 54]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0c36632ea64e3fbbd486a76ef3e9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6896880ab8419c9ee93a34865e336f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba75cfd85934e86be27485900508c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d6c2bc5c7b4cec9305935179cfac54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5248a2bcda5d44c1a42f73c9d2b5f05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_folder = Path('../images/Payoff/M3')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
