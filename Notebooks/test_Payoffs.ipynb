{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import PayoffM1, PayoffM2, PayoffM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (0, 0) is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[1] = 0.36000000000000004\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[1] = 0.488\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[1] = 0.5904\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[1] = 0.67232\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[1] = 0.7378560000000001\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[1] = 0.7902848\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[1] = 0.83222784\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[1] = 0.865782272\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.86578227]\n",
      "Action probabilities:\n",
      "no go:9.636696231312355e-07 ---- go:0.9999990363303768\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.865782272 + 0.2 * (1 - 0.865782272)\n",
      "Q[1] = 0.8926258176\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (1, 1) is: -1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[1] = -0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (0, 0) is: -1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[1] = -0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 0] is: -1\n",
      "Learning rule:\n",
      "Q[1] <- -0.2 + 0.2 * (-1 - -0.2)\n",
      "Q[1] = -0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.36]\n",
      "Action probabilities:\n",
      "no go:0.9968587867151706 ---- go:0.0031412132848294247\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/Payoff/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/Payoff/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da019f3960446838e6b2715361c83f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cac06bfdfc42e6b90f619be0ba3ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2649c785f11f4c6184fb02c61ea40410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd2b445549484caf962d95055fc00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c900779f8a514789b4c694894874c67e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc69c8b30ac48bd910f52cf5928423b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fd1c52c0e94bda88b373a3af5b5d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Payoff/M1/conditional_entropy_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/M1/entropy_learning_rate.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/M1/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Payoff/M1/inequality_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e686d24a12e452881951fc303c80c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccccd7a7e37b497c8e1d4366f25e5157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0045d0875949679552773e14a92527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6765cab0597a494aa9afbceb0eb7e9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX_string = Performer.simple_plots(\n",
    "\tagent_class=QAttendance,\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tsimulation_parameters=simulation_parameters,\n",
    "\tmeasures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "\timage_folder=image_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781ffe5071444731b125b4a2a2f0a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ceb0ac21dd34c7298b17b1fd824b3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57d4f2ae1c24e629966527a5834f113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5d423a3f964a1dab327f9c8c73e8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d343df4d40ac4b24907886393b3937e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb44357ab2b40b3886151122882d551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b8977f42764cceb563d89a9d700fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31763c8635ec4ee1b6746dbbd015970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb2a6ad40be043bb9c49cbf57fbeb9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72e76ad4a964e4796cb20bd2af77765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718041f75adc4b4091bd2c0a4ffa2f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db71d6c900b349dea8d2f91e1d537e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb5637077f74a94b61a1ba4623afd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f24b22002704a9a978ff5ec041c305f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44efc3320cd54da3a3143b4fa53a517f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae7de542c3a4defbf4b592b5356deea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937d693bb9a94ed98c40f2a00f22e895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0daf2347c24a61af2fa1114a1b8c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd47d008ad464b6a9e56af575f518323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cd6acdc9944047acf096d8fca42511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d9d7e85e9f460e853356d076aef895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e62819189541fe939f7f453f9914ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd61a63269f74a2ca2320490c734ef9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155304ae0e2a44558a7c9fdd435cbe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864cfe35e3c4457980d7a765f6c89ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc11ac5104e948f09fb4f5469db9ea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf3b91538e64fff9c031db6a8b5fd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e42141d5f847a6817502f761bf8ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f028d4c770e6447490fb6f5739a59c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfe928c09644b41b9e9852df8bc7826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371efeea0aba473680f0408cac53ff4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b7bfadec0b4c78a95d8e034bcd98d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65121081fb60482a9541a802e4f36327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a099d9187743c4a22dd02efeacb30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e53508d10f43aa988291376dfc6acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c008932131a749d9ad42fcdd39430024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43beb2464de049e49fbc0e42667ac9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3483eb8e194255924cba08de623c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7650ebbc58e7449f886b829757e83cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bc37d3439d43f68d339e76a1088b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3c62e07c8d4a8088b0b609dd014d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62589e220d8c466489785c7bc0b2106d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44e4bec243044a295f18d2c2229ab4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c526f6ee8f9a49bbaa1afa8543e9175c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d326f129b4480c966e6a3f669517da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b05ffdf715e45bf90463672eeebc749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be03769bfb3243f38fbef6f9704c4957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667b79a523f54710a29ad07bb6519479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368bf1be72ad49779514050dfde8a320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c7110d8ffa4b15934fb8c51546b5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M1/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M1/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state (0, 0) is: 1\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(0, 0), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 0), 1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 0), 1] = 0.36000000000000004\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[(1, 0), 1] = 0.488\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[(1, 0), 1] = 0.5904\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[(1, 0), 1] = 0.67232\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[(1, 0), 1] = 0.7378560000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.737856]\n",
      "Action probabilities:\n",
      "no go:7.461877782840652e-06 ---- go:0.9999925381222171\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7378560000000001 + 0.2 * (1 - 0.7378560000000001)\n",
      "Q[(1, 0), 1] = 0.7902848\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7902848]\n",
      "Action probabilities:\n",
      "no go:3.2250567045596994e-06 ---- go:0.9999967749432954\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7902848 + 0.2 * (1 - 0.7902848)\n",
      "Q[(1, 0), 1] = 0.83222784\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.83222784]\n",
      "Action probabilities:\n",
      "no go:1.6484961852716907e-06 ---- go:0.9999983515038148\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.83222784 + 0.2 * (1 - 0.83222784)\n",
      "Q[(1, 0), 1] = 0.865782272\n"
     ]
    }
   ],
   "source": [
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (1, 1) is: 0\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 1), 0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[(0, 1), 1] = -0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [1, 1] is: -1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[(1, 1), 1] = -0.2\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 1), 0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (0, 0) is: -1\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[(0, 0), 1] = -0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(1, 1), 1] = 0.2\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [1, 0] is: -1\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[(1, 0), 1] = -0.2\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(1, 1), 1] = 0.36000000000000004\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 0), 0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 1), 0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 0), 0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[(0, 1), 1] = 0.2\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 0), 0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[(0, 1), 1] = 0.36000000000000004\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/Payoff/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/Payoff/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56316775c5fb4a39a534add197911fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d836da06e9aa49da95c8afd46d405465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c704051e564c209ffe96ca85c68748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc84891a1ab4169be20039a85a1fafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925aa6635891485b890bec6a92236045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada6e36f6c5b41deae6eebb136d22ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f875541fb54bb3a31c437e9e606a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered_models={0.0: 0.0, 0.05: 0.05, 0.1: 0.1, 0.2: 0.2, 0.4: 0.4, 0.8: 0.8}\n",
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M2/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/Payoff/M2/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M2/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/Payoff/M2/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebc304c343d45adbf01de8b8a4c8c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27538ec5aaf44f4db2328af52e4055c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2c1e9bcdc74eabb5515ba189cc42e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cf121a0318406691530f060cccdaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bb611ce8614704af1bc419927ae14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626653e1f6184d2492708bb56bc781c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f80faef06d41a39c22ef2723ed253a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc570f31af24cbfb963ce536567a85e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered_models={1: 1, 2: 2, 4: 4, 8: 8, 16: 16, 32: 32, 64: 64}\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M2/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M2/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08948d86e6e4e028d8272530a64e4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22a54eef02c4c7a82a3d3474c8d9b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9626d7175a0d4c87b8bb1ccdc2124cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc2f48311df41d895864b0a54157a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01d6d532e644fa48b2fdd5bb2201829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68d3045da514fba8e6b9a425220cfdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1382b3a030d64e97a3c8eaba133c1bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e9ae66e5dc41198076f0ef83576a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372c3277bb604922b1149a1f8c49c165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb29b8f831f4e56be572e2e45934ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be03ab4e01204853ae113f9981652437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9070cc1f2c476f917e752ecde49b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792bbd85e6674e579e072d62437b7d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54feb2a5dcc47e3ad2971c8a61585d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd814d26c5ac4ec291fcb5b56bf48040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1069abf39a3140e5964c24a867c04f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50256c150bb74504b7638add07b4f614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d910363635b449d2ac5e0bbc08e7a03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473e274ecc8647df9b5b5c3c877d9c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a03931315ec4f03b26554193d0efb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba59cb4f07444e41b075e2ad304f7837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a104cbacbf944c58a6492d1a5c6fbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f7f9f4a4ae49c79504f14f0d0391fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff8f1cf8d0e47d5b3c0857d5963c5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed468f2b82384967a71f822e8788c51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c607282f474ad3a1a9547f43a59551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f7a3279de1429cbf1b25d9d6b7ca87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3d690e776c418c96b55a760d39c01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f6da5c4e8948ec87d934654ce97658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285995d8b99d49dabdaa697ae5f11508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9764eeb03541099ded356039fdd92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fa16e6a5d144678d960c0ce4a9e4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41da5380ebc6441d83ca4b9266e1caab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1edff7affa6a41c4b4fd41901b7735aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5165d937c7f47559c970d9dcc5d92b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4ca9cd68b54ebf97fa0707c3637c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5001908fd911472387e579a50e51f7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8daa3c7ebe48f2a50058923b7e17c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e8b418013b44adb8c7079735954032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3f6e9f13a745f0ada9db48b788444f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7b74d78a964f4b890c6237d34379c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca31a4553da4868b8fd17bb3f979225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3f122093384982ad17349755f6aa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42b266dfa574fa5b25f4a3ba0193ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21214432838f41bebb64da7d9e1080e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa2d7d3d964146aab7169544fd760f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653f757756bc47548a82a39b0cd08b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669dc7b5dca6447ba51f0a3fd3faf225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79a59151ede441c939829b6f530298e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97113d315334e59888ace4fa41a733c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M2/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M2/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[(0, 0),0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 0),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[0, 0],1] = 0.2\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[1, 0],1] = 0.2\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[[1, 0],1] = 0.36000000000000004\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.36]\n",
      "Action probabilities:\n",
      "no go:0.0031412132848294256 ---- go:0.9968587867151706\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.36000000000000004 + 0.2 * (1 - 0.36000000000000004)\n",
      "Q[[1, 0],1] = 0.488\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.488]\n",
      "Action probabilities:\n",
      "no go:0.0004063050248981527 ---- go:0.9995936949751019\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.488 + 0.2 * (1 - 0.488)\n",
      "Q[[1, 0],1] = 0.5904\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.5904]\n",
      "Action probabilities:\n",
      "no go:7.896712192878896e-05 ---- go:0.9999210328780712\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.5904 + 0.2 * (1 - 0.5904)\n",
      "Q[[1, 0],1] = 0.67232\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.67232]\n",
      "Action probabilities:\n",
      "no go:2.1292805909385266e-05 ---- go:0.9999787071940905\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.67232 + 0.2 * (1 - 0.67232)\n",
      "Q[[1, 0],1] = 0.7378560000000001\n"
     ]
    }
   ],
   "source": [
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (1, 1) is: 0\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(1, 1),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[[0, 1],1] = -0.2\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [1, 1] is: -1\n",
      "Learning rule:\n",
      "Q[[1, 1],1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[[1, 1],1] = -0.2\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 1],0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[(0, 0),0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[(0, 0),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 0] is: -1\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[[0, 0],1] = -0.2\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 1],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[1, 1],1] = 0.2\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [1, 0] is: -1\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (-1 - 0.0)\n",
      "Q[[1, 0],1] = -0.2\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 1]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[1, 1],1] <- 0.2 + 0.2 * (1 - 0.2)\n",
      "Q[[1, 1],1] = 0.36000000000000004\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[1, 0],0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.2]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (1 - 0.0)\n",
      "Q[[0, 1],1] = 0.2\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/Payoff/M3')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/Payoff/M3')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfe1063e4f04ef892cbe1925c40c252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4072c304aeb429893c125f9e74f157c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a808285aad734a3797c4a8d65a4e7b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9aa066428e4886a75c28c64cff205c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e1ac553a6476c87a593a3b628e3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d307d8b25b4cb89f9b61a7435a24be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b48a52df464c37adb9b2c999be1568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered_models={0.0: 0.0, 0.05: 0.05, 0.1: 0.1, 0.2: 0.2, 0.4: 0.4, 0.8: 0.8}\n",
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M3/efficiency_learning_rate_1.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/Payoff/M3/inequality_learning_rate_1.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M3/entropy_learning_rate_1.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/Payoff/M3/conditional_entropy_learning_rate_1.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aa0ff6242e45d28c534d20f215aebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc1e32237d9410ab66de297959d3a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a275543622c144d5848d53fa0b5bcbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72a78192a22434785f697613c8747cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e718898459242128b5304ec06d8d15f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1d0fe224504fd9819abd94c75244f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96117b8f4d89480b9b594b2ae2f6ce61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2263b9811b8a4dff8f8348a1f6fa3ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordered_models={1: 1, 2: 2, 4: 4, 8: 8, 16: 16, 32: 32, 64: 64}\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M3/entropy_inverse_temperature_1.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M3/efficiency_inverse_temperature_1.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afaff3325940dc8803063fac6d213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b2d483de924855ace309a1ad4e7593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e778d6698ad4f1492d0e0eb5a65d153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ba7f1eedce4f11b1f9d2e1c51f061f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5b55c9f2b74f85b72e2d8f77e07820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1eea76be984c9c8e69ccbf8092b760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5bd956a7ab4348b6ffb548e0a85695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e04670824de459cab74db52e65ffcf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0bba389a774930a0f2293a4a0e3b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfadee5e6a294632a89c206cce9b5b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2ff3b4e41543ee9d2fabda7bd00829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb63c3adb5e448599cfb2e2dc977fc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160c3d88c6d447e48a2d509fa7d460e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94dc39b086c46b49055ddbcd7a9e110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9a7342ebfa4353817e67ae67b1348d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8d58d53423419eaa36582d4c095229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e7c2b4dca7423eaaeb91ded8a22199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186f0ea1c3cb444189c924c8998b61e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20784f0c1a2e4aa0a7adbdfa60762dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab7cc063c2f45fc851e21c2a1319c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17392583f2a94e0e99bc53d7bd17ca63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57faf18be63468ca1cf6a7e5f3f5d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07fd85da1aa14070a2978a9d93d5e8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fcadf5ae26474b9f42bfab3a98e80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b79257bbb941c0aec0cc1ab9ab42d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edddde4e5daa4443981d682bf415b4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6927e8fac0ce4cfbb9b29cbbdfb1883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37104fd7bf94154ae0cb753c43abcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07081fd11b641989fd3ce1ad40413f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d165bd6e58794af2911db3331b5a1d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b009108bea41df88e368169bc13ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb3e8177a44bb694d964e1a07dbb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3915897525574d3ab799656124ee588a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4205cbfb79044b1cb6a319fcfa93b2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05cce7f26964f3ab2bf8337fa08efe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db949887f5a24ea89898a39a5f22c2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611f401e88d247dfa4d787d536e51331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0090c2ac88b84523a502600b670f6a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8acc78057c48eabb05b0fdd31c4ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe45335d68a45479257b18bc98eeebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e1ba260a1f494ca558593d723e8e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f7065ee0c449fb852e9f272b9987e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47df3f52afb64285bac44386b3b04a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e0acd42ff4e7cb42b77cd059e0925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57efea4745e4a65a20ec6056f61921d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8741ab9f92854ddb94875255dd6f2f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb4bec071d74a3cb51f600e4645afa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1689be1af0934b74aaf978a15a462520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3cd459cad046e481610cfd48dd3e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07e91aeea8e46389e2e38be569d5a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M3/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M3/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/Payoff')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/Payoff')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': PayoffM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': PayoffM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': PayoffM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbac3d445b5446af863414b60534e430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7675044e94a4ea990bb26193cbc3730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b08be77ed84557bda1ee4ab82b28f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/Payoff/efficiency_2.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Payoff/inequality_2.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Payoff/entropy_2.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Payoff/conditional_entropy_2.pdf\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Payoff-M1-7': 'M1',\n",
    "            'Payoff-M2-9': 'M2',\n",
    "            'Payoff-M3-0': 'M3'\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
