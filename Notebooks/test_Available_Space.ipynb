{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-driven Available Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import AvailableSpaceM1, AvailableSpaceM2, AvailableSpaceM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[0] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [-0.1  0. ]\n",
      "Action probabilities:\n",
      "no go:0.1679816148660755 ---- go:0.8320183851339245\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[1] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [-0.1  0.1]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[1] = 0.18000000000000002\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [-0.1   0.18]\n",
      "Action probabilities:\n",
      "no go:0.011206406321842869 ---- go:0.9887935936781571\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.18000000000000002 + 0.2 * (0.5 - 0.18000000000000002)\n",
      "Q[1] = 0.244\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [-0.1    0.244]\n",
      "Action probabilities:\n",
      "no go:0.004053955551566971 ---- go:0.995946044448433\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.244 + 0.2 * (0.5 - 0.244)\n",
      "Q[1] = 0.2952\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [-0.1     0.2952]\n",
      "Action probabilities:\n",
      "no go:0.0017909795301548417 ---- go:0.9982090204698452\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.2952 + 0.2 * (0.5 - 0.2952)\n",
      "Q[1] = 0.33616\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [-0.1      0.33616]\n",
      "Action probabilities:\n",
      "no go:0.0009307766525170825 ---- go:0.9990692233474829\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.33616 + 0.2 * (0.5 - 0.33616)\n",
      "Q[1] = 0.36892800000000003\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [-0.1       0.368928]\n",
      "Action probabilities:\n",
      "no go:0.0005512074296002179 ---- go:0.9994487925703998\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.36892800000000003 + 0.2 * (0.5 - 0.36892800000000003)\n",
      "Q[1] = 0.3951424\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [-0.1        0.3951424]\n",
      "Action probabilities:\n",
      "no go:0.0003624438799524942 ---- go:0.9996375561200475\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.3951424 + 0.2 * (0.5 - 0.3951424)\n",
      "Q[1] = 0.41611392\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [-0.1         0.41611392]\n",
      "Action probabilities:\n",
      "no go:0.00025915559877418026 ---- go:0.9997408444012257\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.41611392 + 0.2 * (0.5 - 0.41611392)\n",
      "Q[1] = 0.432891136\n"
     ]
    }
   ],
   "source": [
    "agent = AvailableSpaceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state (1, 1) is: -0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[0] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [ 0.1 -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[0] = 0.18000000000000002\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.18 -0.1 ]\n",
      "Action probabilities:\n",
      "no go:0.9887935936781571 ---- go:0.011206406321842869\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.18000000000000002 + 0.2 * (0.5 - 0.18000000000000002)\n",
      "Q[0] = 0.244\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.244 -0.1  ]\n",
      "Action probabilities:\n",
      "no go:0.995946044448433 ---- go:0.004053955551566971\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.244 + 0.2 * (0.5 - 0.244)\n",
      "Q[0] = 0.2952\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.2952 -0.1   ]\n",
      "Action probabilities:\n",
      "no go:0.9982090204698452 ---- go:0.0017909795301548417\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.2952 + 0.2 * (0.5 - 0.2952)\n",
      "Q[0] = 0.33616\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.33616 -0.1    ]\n",
      "Action probabilities:\n",
      "no go:0.9990692233474829 ---- go:0.0009307766525170825\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.33616 + 0.2 * (0.5 - 0.33616)\n",
      "Q[0] = 0.36892800000000003\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.368928 -0.1     ]\n",
      "Action probabilities:\n",
      "no go:0.9994487925703998 ---- go:0.0005512074296002179\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.36892800000000003 + 0.2 * (0.5 - 0.36892800000000003)\n",
      "Q[0] = 0.3951424\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.3951424 -0.1      ]\n",
      "Action probabilities:\n",
      "no go:0.9996375561200475 ---- go:0.0003624438799524942\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.3951424 + 0.2 * (0.5 - 0.3951424)\n",
      "Q[0] = 0.41611392\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.41611392 -0.1       ]\n",
      "Action probabilities:\n",
      "no go:0.9997408444012257 ---- go:0.00025915559877418026\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.41611392 + 0.2 * (0.5 - 0.41611392)\n",
      "Q[0] = 0.432891136\n"
     ]
    }
   ],
   "source": [
    "agent = AvailableSpaceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[0] = -0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [-0.1 -0.1]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state [0, 0] is: -0.5\n",
      "Learning rule:\n",
      "Q[1] <- -0.1 + 0.2 * (-0.5 - -0.1)\n",
      "Q[1] = -0.18000000000000002\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [-0.1  -0.18]\n",
      "Action probabilities:\n",
      "no go:0.7824497764231125 ---- go:0.21755022357688744\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.1 + 0.2 * (-0.5 - -0.1)\n",
      "Q[0] = -0.18000000000000002\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [-0.18 -0.18]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.18000000000000002 + 0.2 * (0.5 - -0.18000000000000002)\n",
      "Q[0] = -0.04400000000000001\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [-0.044 -0.18 ]\n",
      "Action probabilities:\n",
      "no go:0.8980735047412169 ---- go:0.10192649525878313\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.04400000000000001 + 0.2 * (-0.5 - -0.04400000000000001)\n",
      "Q[0] = -0.13520000000000001\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [-0.1352 -0.18  ]\n",
      "Action probabilities:\n",
      "no go:0.6719019664234913 ---- go:0.3280980335765086\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.13520000000000001 + 0.2 * (0.5 - -0.13520000000000001)\n",
      "Q[0] = -0.00816\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.00816 -0.18   ]\n",
      "Action probabilities:\n",
      "no go:0.9398817153366148 ---- go:0.060118284663385174\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.00816 + 0.2 * (-0.5 - -0.00816)\n",
      "Q[0] = -0.10652800000000001\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [-0.106528 -0.18    ]\n",
      "Action probabilities:\n",
      "no go:0.7641470992259752 ---- go:0.23585290077402477\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- -0.10652800000000001 + 0.2 * (0.5 - -0.10652800000000001)\n",
      "Q[0] = 0.014777599999999988\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.0147776 -0.18     ]\n",
      "Action probabilities:\n",
      "no go:0.957565873203747 ---- go:0.042434126796252976\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.014777599999999988 + 0.2 * (-0.5 - 0.014777599999999988)\n",
      "Q[0] = -0.08817792\n"
     ]
    }
   ],
   "source": [
    "agent = AvailableSpaceM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/AvailableSpace/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/AvailableSpace/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d73f87c26e460aa3ed6ef805c127f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa9652c054d4c77b556128e18053c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e211b612e1c84470b6c3d9f7b96bada1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8b2edc2ec84412b08f76d04b758939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce913209127647238c77d946f76f55fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d15019254a49ff8f8a3db5ca6c07dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d3150af137494ab7b578203f160512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/AvailableSpace/M1/conditional_entropy_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/AvailableSpace/M1/entropy_learning_rate.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/AvailableSpace/M1/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/AvailableSpace/M1/inequality_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58109b9421e447248f8b1b5481406c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ed04eb21cf421f82bdfd5c9b5f39ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d0f12a71464a709240165e3e1e748d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ee3c91562d49acad5ffc0625a335db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc42cbbcdf2481c91321dcd53bb220a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7251472f04340cf827a6d2f0af4b799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be14061d065d4299ba1356d8ec1badf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce79e3573e9141858f04489ab2fb4c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../images/AvailableSpace/M1/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/AvailableSpace/M1/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=AvailableSpaceM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d056dba2da54c9e8371f7ace50caa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a790680cc6400bab10b6dbae61f7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4c42c2475e46ae9fbe69f459a1b4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9328be57e9943298e069024cbc5d912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdce91bc3b934e74a7c8aade0ee46662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd92fc55ae2a47e7afe063b8497489e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7735ea01a5c441219b7d69b432054dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341e9199a5a24de28d0e54b76d38e521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28afde46e0f44f31863fa6178b68dcd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5569efb983145e8bb06d638f0c9de83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e7abcf67914a448b6a45832e44baaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdc00cd8756434cb0c5729993e62909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e24811490a442f7adbe80edfd1914e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09ec80fd7c4447db7373f65842ca096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c16b6223fe4f40972767d75700dd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036c900f7b2f47ac885604108d745868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9971184a69dc45c5b31811a1868dc2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ca0e57bdf54eeaa13fb495dc9a5b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc0217a694b451987ef231fbbdf9002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c94c8ed5c5a4b79beafbc8ae3962f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a318f0f9422a427cb0a28866aad06bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45e5f4f4eda4f779322937a9eeab162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70e1f5456a04db5ad54b3546447e3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec69af417584456da55968c5a95ef8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb39b426e9b54a2980bd3469516c3420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a68a5b85944ccfae176557691baa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478ac08b5444602b69e1821417cdfa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079123ea998e4aa49112234b1a07e5f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2ce75a9bc34546a546e06fa1e427ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21d49d9f37f495f8008171fc0f09d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b447ac9c6d488aa108114e81ad74ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e866dd66300347ec8b97fd28ebf80f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db992cf082bd4a41a983a62b0378e7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6910669764db45829b9909577c0c3560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4f319ae7354124ae3ebc39f06c5515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e21d9615ac460ebf34f67cef625dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fccb79684844cbaa8355b319477edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecf3b3307d04d2e912f37ff1ad42795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416f857601a14be6999294a74f760a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7a117bd4b444d78e12e7052e70075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c81ba8e67584f198557de4dfed20020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba47aafe8cf4015838aa06fd3ff61e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4027730c48f046cd9e0214909e65136b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689c710e9f32487a8963fbf1233a967c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32473f61c98a4272812de099543db99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cdb8def5cc4214a158cdfa203e5cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eea2e4a5fa848a29b46d3e5c2be0d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f837e22ae2045a685957330aca79c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a9c6bab5f541bdbf90c82998d7aa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edb1850797440069f4d82a623deaab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/AvailableSpace/M1/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/AvailableSpace/M1/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 0), 0] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [-0.1  0. ]\n",
      "Action probabilities:\n",
      "no go:0.1679816148660755 ---- go:0.8320183851339245\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 0), 1] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 0] is: -0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(1, 0), 0] = -0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [-0.1  0.1]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(0, 0), 1] = 0.18000000000000002\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [-0.1  0. ]\n",
      "Action probabilities:\n",
      "no go:0.1679816148660755 ---- go:0.8320183851339245\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(1, 0), 1] = 0.1\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [-0.1  0.1]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(1, 0), 1] = 0.18000000000000002\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [-0.1   0.18]\n",
      "Action probabilities:\n",
      "no go:0.011206406321842869 ---- go:0.9887935936781571\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.18000000000000002 + 0.2 * (0.5 - 0.18000000000000002)\n",
      "Q[(1, 0), 1] = 0.244\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [-0.1    0.244]\n",
      "Action probabilities:\n",
      "no go:0.004053955551566971 ---- go:0.995946044448433\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.244 + 0.2 * (0.5 - 0.244)\n",
      "Q[(1, 0), 1] = 0.2952\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [-0.1     0.2952]\n",
      "Action probabilities:\n",
      "no go:0.0017909795301548417 ---- go:0.9982090204698452\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.2952 + 0.2 * (0.5 - 0.2952)\n",
      "Q[(1, 0), 1] = 0.33616\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [-0.1      0.33616]\n",
      "Action probabilities:\n",
      "no go:0.0009307766525170825 ---- go:0.9990692233474829\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.33616 + 0.2 * (0.5 - 0.33616)\n",
      "Q[(1, 0), 1] = 0.36892800000000003\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = AvailableSpaceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state (1, 1) is: -0.5\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(1, 1), 1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(1, 1), 0] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 1), 0] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0.1 0. ]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339246 ---- go:0.16798161486607552\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state [0, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 1), 1] = -0.1\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.1 -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(1, 1), 0] = 0.18000000000000002\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.1 -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(0, 1), 0] = 0.18000000000000002\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.18 -0.1 ]\n",
      "Action probabilities:\n",
      "no go:0.9887935936781571 ---- go:0.011206406321842869\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.18000000000000002 + 0.2 * (0.5 - 0.18000000000000002)\n",
      "Q[(0, 1), 0] = 0.244\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.244 -0.1  ]\n",
      "Action probabilities:\n",
      "no go:0.995946044448433 ---- go:0.004053955551566971\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.244 + 0.2 * (0.5 - 0.244)\n",
      "Q[(0, 1), 0] = 0.2952\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.2952 -0.1   ]\n",
      "Action probabilities:\n",
      "no go:0.9982090204698452 ---- go:0.0017909795301548417\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.2952 + 0.2 * (0.5 - 0.2952)\n",
      "Q[(0, 1), 0] = 0.33616\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.33616 -0.1    ]\n",
      "Action probabilities:\n",
      "no go:0.9990692233474829 ---- go:0.0009307766525170825\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.33616 + 0.2 * (0.5 - 0.33616)\n",
      "Q[(0, 1), 0] = 0.36892800000000003\n"
     ]
    }
   ],
   "source": [
    "agent = AvailableSpaceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 0), 1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(1, 1), 0] = -0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 0), 0] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 0\n",
      "G = 0.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 1] is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 1), 0] = -0.1\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [ 0.1 -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [0, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(0, 0), 0] = 0.18000000000000002\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [-0.1  0. ]\n",
      "Action probabilities:\n",
      "no go:0.1679816148660755 ---- go:0.8320183851339245\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 1), 1] = 0.1\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Attendance other players: 1\n",
      "G = 0.5 * 2 - 1.5\n",
      "G observed for action 1 in state [1, 0] is: -0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(1, 0), 1] = -0.1\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [-0.1  0. ]\n",
      "Action probabilities:\n",
      "no go:0.1679816148660755 ---- go:0.8320183851339245\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(1, 1), 1] = 0.1\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Attendance other players: 1\n",
      "G = 1.5 - 0.5 * 2\n",
      "G observed for action 0 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(1, 0), 0] = 0.1\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [-0.1  0.1]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Attendance other players: 0\n",
      "G = 0.5 * 2 - 0.5\n",
      "G observed for action 1 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[(0, 1), 1] = 0.18000000000000002\n"
     ]
    }
   ],
   "source": [
    "agent = AvailableSpaceM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/AvailableSpace/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/AvailableSpace/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a901f8c2c14c0f8cad2bbfa1f77ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf00e715d24b4cda86a4ffa93ac34b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a7c8cf445746368644fccb3681d504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2938c3cf4f254358875850479ca9507f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57effbd9d90f43f9ba8fd011d862350a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1652a6f11048f38221f624c4fb4dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d320a89a22f426a822155678259f88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/AvailableSpace/M2/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/AvailableSpace/M2/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/AvailableSpace/M2/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/AvailableSpace/M2/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068948600cd74764896f618596356c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6688adce38e44c56baa32c087c326f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8629cb420244c5588f3e86ac2fbd85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acffbfeb308b4e179ec33032928dbd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc384fc2ea54737aa84e1d3703c0eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c0800edeb74bd7a4de15ab7bd9d0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ee429dbcd54c84bf35bebe19661ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172c3b7108014fcabb961a3efb11ae57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../images/AvailableSpace/M2/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/AvailableSpace/M2/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=AvailableSpaceM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13ab34ee4454a4a890b1aa7b5b95b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6175983628334b83ba15fe289bef8f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea550a5340e4a229a15f792b19c775d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce9b4767db5440fa751b65b4ca8eeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ebe59a97164e52bd17b8d653b6e96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7345c8ca3f7340069550717d0bceef89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89973672a314f3db1ab2fa29183083d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf6c1da78bf471688be05a4a678b807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7db570a9e19446abde78d57162c14da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd30f9d5721a402c8c4c5fb8548d017a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd54b44028543b8aaa966b7f1662fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215f32bdeb434418a57e533ea78d51bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218ced604be543e18376e14ff28a1c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9e8e227073411e857ce7a1375e7daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bd851053804e7d8a77ec6286c90a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1452e7b99de04d83a8e09e60abdf9ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5db3a77983d410c8afbb9437d5323f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a729e06d20045beacec3378c95dd0ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99678222a53425ba1c7966587fac031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa23b4d6f874340baac768d734612c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587304fb489d4bba80d8d7926ac32fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b0ab3a339e43419245005500271e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966ec8cfa32b4b56a2289246c5a96043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b96420facde45aa908b9080f7577195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f90e794f80647e0b957199a06358ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6845031913d5443ea30fb4647f16b489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dabd18a5d2c4cbfadb5240d214ff223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16036244a2474b64a35ca0fe2a9c5c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faceb0a21dac46ac84a56606babf829d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb336dee0524dcb8e368fd78ebc4bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3615492b162c40b1b7e767e059fd364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33abae4d6d9a402a8cdcba1a17095882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fdf67b4f3b47ca8ee9568add5e4350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c3a8ed65054729a7ffecdf889c1d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ae0a8801ca46e483a1e383e1c06504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3328d55802c244448f0011b8e3a1730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631ca4f83c0d465d96ee3711a3efb94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7971d79b35e74feea61d35c44733a9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45ef25a83be4385b03a6618751a1dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21051c9311e4bb68d249b657b5908aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461e19cbab7f4b8b9932b9a516517cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4cbaf0a167842d78b5addd6b5213cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed62f09eb947454fba1284d6e8828270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66155f897474b68a8a198c181473b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608410c0e357424a82ff460af47f7cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6dfb7e874e54867a62611cc2fde83f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e762a65dc9041cd8bbe98e4559d4cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0ef4d7a6944493ba4113f91a12aa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593a69042a974001a3e81b4097e05b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c49939255d477f9abfa279ba2ab268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/AvailableSpace/M2/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../images/AvailableSpace/M2/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = AvailableSpaceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AvailableSpaceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AvailableSpaceM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/AvailableSpace/M3')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/AvailableSpace/M3')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=AvailableSpaceM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(0, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=AvailableSpaceM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(0, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0, 0.05, 0.1, 0.2, 0.4, 0.8],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../images/AvailableSpace')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../data/AvailableSpace')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': AvailableSpaceM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': AvailableSpaceM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': AvailableSpaceM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Payoff-M1-7': 'M1',\n",
    "            'Payoff-M2-9': 'M2',\n",
    "            'Payoff-M3-0': 'M3'\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
