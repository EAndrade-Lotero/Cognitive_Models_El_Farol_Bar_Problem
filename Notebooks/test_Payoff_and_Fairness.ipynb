{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-driven Payoff and Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import FairnessM1, FairnessM2, FairnessM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "    \"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder_all = Path('../images/Fairness')\n",
    "image_folder_all.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M1 = Path('../images/Fairness/M1')\n",
    "image_folder_M1.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M2 = Path('../images/Fairness/M2')\n",
    "image_folder_M2.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M3 = Path('../images/Fairness/M3')\n",
    "image_folder_M3.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: -0.25\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (-0.25 - 0.0)\n",
      "Q[0] = -0.05\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [-0.05  0.  ]\n",
      "Action probabilities:\n",
      "no go:0.31002551887238755 ---- go:0.6899744811276125\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.25\n",
      "Learning rule:\n",
      "Q[0] <- -0.05 + 0.2 * (-0.25 - -0.05)\n",
      "Q[0] = -0.09000000000000001\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [-0.09  0.  ]\n",
      "Action probabilities:\n",
      "no go:0.19154534856146746 ---- go:0.8084546514385326\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.5\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.25\n",
      "Learning rule:\n",
      "Q[0] <- -0.09000000000000001 + 0.2 * (-0.25 - -0.09000000000000001)\n",
      "Q[0] = -0.122\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [-0.122  0.   ]\n",
      "Action probabilities:\n",
      "no go:0.12433544225336304 ---- go:0.875664557746637\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.3\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.65\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (0.65 - 0.0)\n",
      "Q[1] = 0.13\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [-0.122  0.13 ]\n",
      "Action probabilities:\n",
      "no go:0.017429635703758693 ---- go:0.9825703642962413\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.16666666666666669\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.5833333333333334\n",
      "Learning rule:\n",
      "Q[1] <- 0.13 + 0.2 * (0.5833333333333334 - 0.13)\n",
      "Q[1] = 0.22066666666666668\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [-0.122       0.22066667]\n",
      "Action probabilities:\n",
      "no go:0.00414100705207349 ---- go:0.9958589929479266\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.07142857142857145\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.5357142857142857\n",
      "Learning rule:\n",
      "Q[1] <- 0.22066666666666668 + 0.2 * (0.5357142857142857 - 0.22066666666666668)\n",
      "Q[1] = 0.28367619047619047\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [-0.122       0.28367619]\n",
      "Action probabilities:\n",
      "no go:0.001515007024455794 ---- go:0.9984849929755443\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.28367619047619047 + 0.2 * (0.5 - 0.28367619047619047)\n",
      "Q[1] = 0.3269409523809524\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [-0.122       0.32694095]\n",
      "Action probabilities:\n",
      "no go:0.0007587681505296793 ---- go:0.9992412318494703\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.05555555555555558\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.4722222222222222\n",
      "Learning rule:\n",
      "Q[1] <- 0.3269409523809524 + 0.2 * (0.4722222222222222 - 0.3269409523809524)\n",
      "Q[1] = 0.35599720634920634\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [-0.122       0.35599721]\n",
      "Action probabilities:\n",
      "no go:0.00047679105334746404 ---- go:0.9995232089466525\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.09999999999999998\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.45\n",
      "Learning rule:\n",
      "Q[1] <- 0.35599720634920634 + 0.2 * (0.45 - 0.35599720634920634)\n",
      "Q[1] = 0.3747977650793651\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [-0.122       0.37479777]\n",
      "Action probabilities:\n",
      "no go:0.0003529736050046093 ---- go:0.9996470263949955\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.13636363636363635\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.4318181818181818\n",
      "Learning rule:\n",
      "Q[1] <- 0.3747977650793651 + 0.2 * (0.4318181818181818 - 0.3747977650793651)\n",
      "Q[1] = 0.3862018484271284\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (1, 1) is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.16666666666666663\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.5833333333333333\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.5833333333333333 - 0.0)\n",
      "Q[1] = -0.11666666666666665\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [ 0.         -0.11666667]\n",
      "Action probabilities:\n",
      "no go:0.8660721116759263 ---- go:0.13392788832407368\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.11666667]\n",
      "Action probabilities:\n",
      "no go:0.8660721116759263 ---- go:0.13392788832407368\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.09999999999999998\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.55\n",
      "Learning rule:\n",
      "Q[1] <- -0.11666666666666665 + 0.2 * (-0.55 - -0.11666666666666665)\n",
      "Q[1] = -0.20333333333333334\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.         -0.20333333]\n",
      "Action probabilities:\n",
      "no go:0.962792706737999 ---- go:0.037207293262001055\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.20333333]\n",
      "Action probabilities:\n",
      "no go:0.962792706737999 ---- go:0.037207293262001055\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.07142857142857145\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.035714285714285726\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (-0.035714285714285726 - 0.0)\n",
      "Q[0] = -0.007142857142857145\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [-0.00714286 -0.20333333]\n",
      "Action probabilities:\n",
      "no go:0.9584749918337717 ---- go:0.04152500816622833\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.125\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.0625\n",
      "Learning rule:\n",
      "Q[0] <- -0.007142857142857145 + 0.2 * (-0.0625 - -0.007142857142857145)\n",
      "Q[0] = -0.018214285714285718\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.01821429 -0.20333333]\n",
      "Action probabilities:\n",
      "no go:0.950823134247405 ---- go:0.04917686575259484\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[0] <- -0.018214285714285718 + 0.2 * (-0.08333333333333334 - -0.018214285714285718)\n",
      "Q[0] = -0.031238095238095245\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [-0.0312381  -0.20333333]\n",
      "Action probabilities:\n",
      "no go:0.9401120531250665 ---- go:0.059887946874933456\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.09999999999999998\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.45\n",
      "Learning rule:\n",
      "Q[1] <- -0.20333333333333334 + 0.2 * (-0.45 - -0.20333333333333334)\n",
      "Q[1] = -0.2526666666666667\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [-0.0312381  -0.25266667]\n",
      "Action probabilities:\n",
      "no go:0.9718828931979956 ---- go:0.028117106802004427\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.13636363636363635\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: -0.06818181818181818\n",
      "Learning rule:\n",
      "Q[0] <- -0.031238095238095245 + 0.2 * (-0.06818181818181818 - -0.031238095238095245)\n",
      "Q[0] = -0.03862683982683983\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (-0.08333333333333334 - 0.0)\n",
      "Q[0] = -0.01666666666666667\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [-0.01666667 -0.1       ]\n",
      "Action probabilities:\n",
      "no go:0.791391472673955 ---- go:0.20860852732604493\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 0] is: -0.5\n",
      "Learning rule:\n",
      "Q[1] <- -0.1 + 0.2 * (-0.5 - -0.1)\n",
      "Q[1] = -0.18000000000000002\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [-0.01666667 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.9317147762108838 ---- go:0.06828522378911615\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.09999999999999998\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: -0.04999999999999999\n",
      "Learning rule:\n",
      "Q[0] <- -0.01666666666666667 + 0.2 * (-0.04999999999999999 - -0.01666666666666667)\n",
      "Q[0] = -0.023333333333333334\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [-0.02333333 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.9246078585865676 ---- go:0.07539214141343244\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[0] <- -0.023333333333333334 + 0.2 * (-0.08333333333333334 - -0.023333333333333334)\n",
      "Q[0] = -0.03533333333333334\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [-0.03533333 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.9100844635634817 ---- go:0.08991553643651824\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.2142857142857143\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.10714285714285715\n",
      "Learning rule:\n",
      "Q[0] <- -0.03533333333333334 + 0.2 * (-0.10714285714285715 - -0.03533333333333334)\n",
      "Q[0] = -0.049695238095238105\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [-0.04969524 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.8894245109835927 ---- go:0.1105754890164074\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.25\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.125\n",
      "Learning rule:\n",
      "Q[0] <- -0.049695238095238105 + 0.2 * (-0.125 - -0.049695238095238105)\n",
      "Q[0] = -0.06475619047619048\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.06475619 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.8634094133078541 ---- go:0.13659058669214585\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.2777777777777778\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.1388888888888889\n",
      "Learning rule:\n",
      "Q[0] <- -0.06475619047619048 + 0.2 * (-0.1388888888888889 - -0.06475619047619048)\n",
      "Q[0] = -0.07958273015873016\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [-0.07958273 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.8329494253203704 ---- go:0.1670505746796296\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.3\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.15\n",
      "Learning rule:\n",
      "Q[0] <- -0.07958273015873016 + 0.2 * (-0.15 - -0.07958273015873016)\n",
      "Q[0] = -0.09366618412698413\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [-0.09366618 -0.18      ]\n",
      "Action probabilities:\n",
      "no go:0.7992062930362402 ---- go:0.20079370696375978\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.2272727272727273\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.6136363636363636\n",
      "Learning rule:\n",
      "Q[1] <- -0.18000000000000002 + 0.2 * (0.6136363636363636 - -0.18000000000000002)\n",
      "Q[1] = -0.021272727272727276\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df233076f63747b1ba53339330848d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6b08ec3dc6497fb6c971ba30e9b560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3452b6d0a94c93b04d094a35d3e117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49d9d21eeeaf4a99af4571b5c3ccadc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6151e0a12934d60bbef12ecde60d602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M1\\conditional_entropy_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M1\\entropy_bias.png\n",
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Fairness\\M1\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Fairness\\M1\\inequality_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 0), 1] = 0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.16666666666666663\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.4166666666666667\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (0.4166666666666667 - 0.0)\n",
      "Q[(1, 0), 1] = 0.08333333333333334\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.08333333]\n",
      "Action probabilities:\n",
      "no go:0.2086085273260449 ---- go:0.791391472673955\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.25\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.375\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.08333333333333334 + 0.2 * (0.375 - 0.08333333333333334)\n",
      "Q[(1, 0), 1] = 0.14166666666666666\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.14166667]\n",
      "Action probabilities:\n",
      "no go:0.09392149601927116 ---- go:0.9060785039807289\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.30000000000000004\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.35\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.14166666666666666 + 0.2 * (0.35 - 0.14166666666666666)\n",
      "Q[(1, 0), 1] = 0.18333333333333332\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.18333333]\n",
      "Action probabilities:\n",
      "no go:0.05053016223541346 ---- go:0.9494698377645866\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.33333333333333337\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3333333333333333\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.18333333333333332 + 0.2 * (0.3333333333333333 - 0.18333333333333332)\n",
      "Q[(1, 0), 1] = 0.21333333333333332\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.21333333]\n",
      "Action probabilities:\n",
      "no go:0.031881353685947696 ---- go:0.9681186463140523\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.2142857142857143\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.10714285714285715\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0.10714285714285715 - 0.0)\n",
      "Q[(1, 0), 0] = 0.021428571428571432\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.16798161486607552 ---- go:0.8320183851339246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.25\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 0] is: 0.375\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.1 + 0.2 * (0.375 - 0.1)\n",
      "Q[(0, 0), 1] = 0.15500000000000003\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.02142857 0.21333333]\n",
      "Action probabilities:\n",
      "no go:0.044341644418290624 ---- go:0.9556583555817094\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.2777777777777778\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3611111111111111\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.21333333333333332 + 0.2 * (0.3611111111111111 - 0.21333333333333332)\n",
      "Q[(1, 0), 1] = 0.24288888888888888\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.02142857 0.24288889]\n",
      "Action probabilities:\n",
      "no go:0.028103229983573227 ---- go:0.9718967700164267\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.30000000000000004\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.35\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.24288888888888888 + 0.2 * (0.35 - 0.24288888888888888)\n",
      "Q[(1, 0), 1] = 0.2643111111111111\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.02142857 0.26431111]\n",
      "Action probabilities:\n",
      "no go:0.020112019562680303 ---- go:0.9798879804373197\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.31818181818181823\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3409090909090909\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.2643111111111111 + 0.2 * (0.3409090909090909 - 0.2643111111111111)\n",
      "Q[(1, 0), 1] = 0.2796307070707071\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (1, 1) is: 0.0\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[(1, 1), 0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.16666666666666663\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.5833333333333333\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (-0.5833333333333333 - 0.0)\n",
      "Q[(0, 1), 1] = -0.11666666666666665\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.25\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 1] is: -0.625\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (-0.625 - 0.0)\n",
      "Q[(1, 1), 1] = -0.125\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [ 0.    -0.125]\n",
      "Action probabilities:\n",
      "no go:0.8807970779778823 ---- go:0.11920292202211755\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.30000000000000004\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 1] is: -0.65\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- -0.125 + 0.2 * (-0.65 - -0.125)\n",
      "Q[(1, 1), 1] = -0.23\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.   -0.23]\n",
      "Action probabilities:\n",
      "no go:0.9753975715972605 ---- go:0.024602428402739435\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.16666666666666663\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.08333333333333331\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.08333333333333331 - 0.0)\n",
      "Q[(1, 1), 0] = 0.016666666666666663\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.11666667]\n",
      "Action probabilities:\n",
      "no go:0.8660721116759263 ---- go:0.13392788832407368\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0714285714285714\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.0357142857142857\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0.0357142857142857 - 0.0)\n",
      "Q[(0, 1), 0] = 0.00714285714285714\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.00714286 -0.11666667]\n",
      "Action probabilities:\n",
      "no go:0.878782649858654 ---- go:0.12121735014134594\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.125\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.5625\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- -0.11666666666666665 + 0.2 * (-0.5625 - -0.11666666666666665)\n",
      "Q[(0, 1), 1] = -0.2058333333333333\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [ 0.01666667 -0.23      ]\n",
      "Action probabilities:\n",
      "no go:0.9810471585726694 ---- go:0.018952841427330513\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.05555555555555558\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.02777777777777779\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.016666666666666663 + 0.2 * (0.02777777777777779 - 0.016666666666666663)\n",
      "Q[(1, 1), 0] = 0.01888888888888889\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.00714286 -0.20583333]\n",
      "Action probabilities:\n",
      "no go:0.9679418026994502 ---- go:0.03205819730054985\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.00714285714285714 + 0.2 * (0.0 - 0.00714285714285714)\n",
      "Q[(0, 1), 0] = 0.005714285714285712\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.00571429 -0.20583333]\n",
      "Action probabilities:\n",
      "no go:0.967224898165373 ---- go:0.03277510183462693\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.04545454545454547\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.022727272727272735\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.005714285714285712 + 0.2 * (-0.022727272727272735 - 0.005714285714285712)\n",
      "Q[(0, 1), 0] = 2.5974025974021334e-05\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 0), 1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (-0.08333333333333334 - 0.0)\n",
      "Q[(1, 1), 0] = -0.01666666666666667\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.25\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.125\n",
      "Learning rule:\n",
      "Q[(0, 0), 0] <- 0.0 + 0.2 * (-0.125 - 0.0)\n",
      "Q[(0, 0), 0] = -0.025\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.09999999999999998\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.55\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (0.55 - 0.0)\n",
      "Q[(0, 1), 1] = 0.11000000000000001\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (-0.08333333333333334 - 0.0)\n",
      "Q[(1, 0), 0] = -0.01666666666666667\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.   0.11]\n",
      "Action probabilities:\n",
      "no go:0.14679033980138234 ---- go:0.8532096601986177\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.07142857142857145\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.5357142857142857\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.11000000000000001 + 0.2 * (0.5357142857142857 - 0.11000000000000001)\n",
      "Q[(0, 1), 1] = 0.19514285714285717\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [-0.01666667  0.        ]\n",
      "Action probabilities:\n",
      "no go:0.4337256058045608 ---- go:0.5662743941954392\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.125\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: -0.0625\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- -0.01666666666666667 + 0.2 * (-0.0625 - -0.01666666666666667)\n",
      "Q[(1, 0), 0] = -0.025833333333333337\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.19514286]\n",
      "Action probabilities:\n",
      "no go:0.04219729412448503 ---- go:0.9578027058755151\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.05555555555555558\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.5277777777777778\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.19514285714285717 + 0.2 * (0.5277777777777778 - 0.19514285714285717)\n",
      "Q[(0, 1), 1] = 0.2616698412698413\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [-0.02583333  0.        ]\n",
      "Action probabilities:\n",
      "no go:0.39811312031994345 ---- go:0.6018868796800566\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.5\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(1, 0), 1] = -0.1\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 1]: [-0.01666667  0.        ]\n",
      "Action probabilities:\n",
      "no go:0.4337256058045608 ---- go:0.5662743941954392\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.045454545454545414\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 1] is: 0.4772727272727273\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (0.4772727272727273 - 0.0)\n",
      "Q[(1, 1), 1] = 0.09545454545454546\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9306b2348a48cfa374b178f9d37ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69afb8ae2b141a3b3f70301eeef7f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d8626245204f32a3ae96d5a64a204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bda4929214744308dc9b6c02e57beda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "573b3ee3382b424b9eb2c91347c50b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Fairness\\M2\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Fairness\\M2\\inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M2\\entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M2\\conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 0.5\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(0, 0),1] = 0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.16666666666666663\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.4166666666666667\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (0.4166666666666667 - 0.0)\n",
      "Q[[1, 0],1] = 0.08333333333333334\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.08333333]\n",
      "Action probabilities:\n",
      "no go:0.2086085273260449 ---- go:0.791391472673955\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.25\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.375\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.08333333333333334 + 0.2 * (0.375 - 0.08333333333333334)\n",
      "Q[[1, 0],1] = 0.14166666666666666\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.14166667]\n",
      "Action probabilities:\n",
      "no go:0.09392149601927116 ---- go:0.9060785039807289\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.30000000000000004\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.35\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.14166666666666666 + 0.2 * (0.35 - 0.14166666666666666)\n",
      "Q[[1, 0],1] = 0.18333333333333332\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.18333333]\n",
      "Action probabilities:\n",
      "no go:0.05053016223541346 ---- go:0.9494698377645866\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.33333333333333337\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3333333333333333\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.18333333333333332 + 0.2 * (0.3333333333333333 - 0.18333333333333332)\n",
      "Q[[1, 0],1] = 0.21333333333333332\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.21333333]\n",
      "Action probabilities:\n",
      "no go:0.031881353685947696 ---- go:0.9681186463140523\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.3571428571428571\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.32142857142857145\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.21333333333333332 + 0.2 * (0.32142857142857145 - 0.21333333333333332)\n",
      "Q[[1, 0],1] = 0.23495238095238094\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.23495238]\n",
      "Action probabilities:\n",
      "no go:0.022770891492384124 ---- go:0.9772291085076159\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.375\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3125\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.23495238095238094 + 0.2 * (0.3125 - 0.23495238095238094)\n",
      "Q[[1, 0],1] = 0.2504619047619048\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.2504619]\n",
      "Action probabilities:\n",
      "no go:0.017856138100875798 ---- go:0.9821438618991243\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.38888888888888884\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3055555555555556\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.2504619047619048 + 0.2 * (0.3055555555555556 - 0.2504619047619048)\n",
      "Q[[1, 0],1] = 0.26148063492063495\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.26148063]\n",
      "Action probabilities:\n",
      "no go:0.015013322254335673 ---- go:0.9849866777456643\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.4\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.3\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.26148063492063495 + 0.2 * (0.3 - 0.26148063492063495)\n",
      "Q[[1, 0],1] = 0.26918450793650794\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.26918451]\n",
      "Action probabilities:\n",
      "no go:0.013295406115011933 ---- go:0.9867045938849881\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: -0.40909090909090906\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.29545454545454547\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.26918450793650794 + 0.2 * (0.29545454545454547 - 0.26918450793650794)\n",
      "Q[[1, 0],1] = 0.27443851544011544\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.5\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (1, 1) is: -0.75\n",
      "Learning rule:\n",
      "Q[(1, 1),1] <- 0.0 + 0.2 * (-0.75 - 0.0)\n",
      "Q[(1, 1),1] = -0.15000000000000002\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.   -0.15]\n",
      "Action probabilities:\n",
      "no go:0.9168273035060777 ---- go:0.08317269649392234\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.16666666666666663\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.08333333333333331\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0.08333333333333331 - 0.0)\n",
      "Q[[1, 1],0] = 0.016666666666666663\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0.0 - 0.0)\n",
      "Q[[0, 1],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.09999999999999998\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.55\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (-0.55 - 0.0)\n",
      "Q[[0, 1],1] = -0.11000000000000001\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.01666667 -0.15      ]\n",
      "Action probabilities:\n",
      "no go:0.9350308308713359 ---- go:0.06496916912866404\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.016666666666666663 + 0.2 * (0.0 - 0.016666666666666663)\n",
      "Q[[1, 1],0] = 0.01333333333333333\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.11]\n",
      "Action probabilities:\n",
      "no go:0.8532096601986177 ---- go:0.14679033980138234\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.07142857142857145\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.035714285714285726\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (-0.035714285714285726 - 0.0)\n",
      "Q[[0, 1],0] = -0.007142857142857145\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [-0.00714286 -0.11      ]\n",
      "Action probabilities:\n",
      "no go:0.8383109823005379 ---- go:0.16168901769946206\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.125\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.0625\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.007142857142857145 + 0.2 * (-0.0625 - -0.007142857142857145)\n",
      "Q[[0, 1],0] = -0.018214285714285718\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.01821429 -0.11      ]\n",
      "Action probabilities:\n",
      "no go:0.8128401530982906 ---- go:0.1871598469017094\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.018214285714285718 + 0.2 * (-0.08333333333333334 - -0.018214285714285718)\n",
      "Q[[0, 1],0] = -0.031238095238095245\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [-0.0312381 -0.11     ]\n",
      "Action probabilities:\n",
      "no go:0.7790588954525817 ---- go:0.22094110454741825\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.2\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.1\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.031238095238095245 + 0.2 * (-0.1 - -0.031238095238095245)\n",
      "Q[[0, 1],0] = -0.0449904761904762\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [-0.04499048 -0.11      ]\n",
      "Action probabilities:\n",
      "no go:0.738879407021667 ---- go:0.2611205929783329\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.13636363636363635\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.4318181818181818\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.11000000000000001 + 0.2 * (-0.4318181818181818 - -0.11000000000000001)\n",
      "Q[[0, 1],1] = -0.1743636363636364\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: -0.0\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (0, 0) is: -0.5\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.0 + 0.2 * (-0.5 - 0.0)\n",
      "Q[(0, 0),1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.16666666666666669\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: -0.08333333333333334\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (-0.08333333333333334 - 0.0)\n",
      "Q[[1, 1],0] = -0.01666666666666667\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.25\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.125\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (-0.125 - 0.0)\n",
      "Q[[0, 0],0] = -0.025\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.3\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.15\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (-0.15 - 0.0)\n",
      "Q[[0, 1],0] = -0.03\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [-0.025 -0.1  ]\n",
      "Action probabilities:\n",
      "no go:0.7685247834990178 ---- go:0.23147521650098232\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.33333333333333337\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.16666666666666669\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- -0.025 + 0.2 * (-0.16666666666666669 - -0.025)\n",
      "Q[[0, 0],0] = -0.053333333333333344\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [-0.03  0.  ]\n",
      "Action probabilities:\n",
      "no go:0.38225212523075097 ---- go:0.617747874769249\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.35714285714285715\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: -0.17857142857142858\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.03 + 0.2 * (-0.17857142857142858 - -0.03)\n",
      "Q[[0, 1],0] = -0.05971428571428572\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [-0.05333333 -0.1       ]\n",
      "Action probabilities:\n",
      "no go:0.6784519491420181 ---- go:0.3215480508579818\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.375\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: -0.1875\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- -0.053333333333333344 + 0.2 * (-0.1875 - -0.053333333333333344)\n",
      "Q[[0, 0],0] = -0.08016666666666668\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.05971429  0.        ]\n",
      "Action probabilities:\n",
      "no go:0.27779440396027744 ---- go:0.7222055960397226\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.2777777777777778\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.6388888888888888\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.2 * (0.6388888888888888 - 0.0)\n",
      "Q[[0, 1],1] = 0.12777777777777777\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: -0.3\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: -0.15\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.2 * (-0.15 - 0.0)\n",
      "Q[[1, 0],0] = -0.03\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [-0.05971429  0.12777778]\n",
      "Action probabilities:\n",
      "no go:0.047431610225961686 ---- go:0.9525683897740382\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.2272727272727273\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.6136363636363636\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.12777777777777777 + 0.2 * (0.6136363636363636 - 0.12777777777777777)\n",
      "Q[[0, 1],1] = 0.22494949494949495\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8293df22e6a04438bf980589016986b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b886ae6d31649db9bb659e3a55629b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8917589656f411fa8575992b4307547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c52d601cac412b92f32480c30d61ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b691d8307d4400aa3138d3db47b3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ..\\images\\Fairness\\M3\\efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ..\\images\\Fairness\\M3\\inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M3\\entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ..\\images\\Fairness\\M3\\conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': FairnessM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': FairnessM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': FairnessM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4629ab8db51402f83c1c022a4c6cd2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder_all,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Attendance-M1-7': 'M1',\n",
    "            'Attendance-M2-9': 'M2',\n",
    "            'Attendance-M3-0': 'M3'\n",
    "        },\n",
    "        'figsize': (3.5, 3)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':1,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [18, 27, 3, 6]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ba79da965c45f69a194e549e3c82bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4d561f2a1843cb86ed7766aa2e62de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406e49698cb2410fb296dfd1ebb4c8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0a76245e5241a1a9ae40a229919562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9738df3dd5424f8aa98430af06c8584b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [58, 96, 73, 31]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8314b9f9f12f4584aa8a59a8a78d9610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e4990bef60473298e476f2bb528463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0de461773d24e2b87c4d5666e25b5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb03b5cb43642f2982b52e617e57f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c52d2bed5ec4324b3abb432a8143e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds chosen for simple simulation: [11, 23, 75, 14]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a4218596b4daf8abaa5c417561fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running seeds...:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255ace76fd7d46d58254f81348d2407c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a837ac68cb7143ee9216e7d6cd5faf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a531ce715e406aa2a5f37e269974b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f999136958454b979da24621e3e61472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
