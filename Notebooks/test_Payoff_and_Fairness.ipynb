{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error-driven Payoff and Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import FairnessM1, FairnessM2, FairnessM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "    \"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder_all = Path('../images/Fairness')\n",
    "image_folder_all.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M1 = Path('../images/Fairness/M1')\n",
    "image_folder_M1.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M2 = Path('../images/Fairness/M2')\n",
    "image_folder_M2.mkdir(parents=True, exist_ok=True)\n",
    "image_folder_M3 = Path('../images/Fairness/M3')\n",
    "image_folder_M3.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [M1](#m1)\n",
    "2. [M2](#m2)\n",
    "3. [M3](#m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 <a class=\"anchor\" id=\"m1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 1.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 1.0\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (1.0 - 0.0)\n",
      "Q[1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.923240862445307\n",
      "Learning rule:\n",
      "Q[1] <- 0.2 + 0.2 * (0.923240862445307 - 0.2)\n",
      "Q[1] = 0.34464817248906143\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.34464817]\n",
      "Action probabilities:\n",
      "no go:0.004012298023114242 ---- go:0.9959877019768858\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7788007830714049\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8894003915357025\n",
      "Learning rule:\n",
      "Q[1] <- 0.34464817248906143 + 0.2 * (0.8894003915357025 - 0.34464817248906143)\n",
      "Q[1] = 0.45359861629838966\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.45359862]\n",
      "Action probabilities:\n",
      "no go:0.0007043167136440384 ---- go:0.9992956832863559\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7408182206817179\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8704091103408589\n",
      "Learning rule:\n",
      "Q[1] <- 0.45359861629838966 + 0.2 * (0.8704091103408589 - 0.45359861629838966)\n",
      "Q[1] = 0.5369607151068835\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.53696072]\n",
      "Action probabilities:\n",
      "no go:0.00018566675597250058 ---- go:0.9998143332440276\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7165313105737893\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8582656552868946\n",
      "Learning rule:\n",
      "Q[1] <- 0.5369607151068835 + 0.2 * (0.8582656552868946 - 0.5369607151068835)\n",
      "Q[1] = 0.6012217031428857\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.6012217]\n",
      "Action probabilities:\n",
      "no go:6.641327041770478e-05 ---- go:0.9999335867295822\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6996725373751304\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8498362686875651\n",
      "Learning rule:\n",
      "Q[1] <- 0.6012217031428857 + 0.2 * (0.8498362686875651 - 0.6012217031428857)\n",
      "Q[1] = 0.6509446162518215\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.65094462]\n",
      "Action probabilities:\n",
      "no go:2.9975090569051404e-05 ---- go:0.9999700249094309\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6872892787909722\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8436446393954862\n",
      "Learning rule:\n",
      "Q[1] <- 0.6509446162518215 + 0.2 * (0.8436446393954862 - 0.6509446162518215)\n",
      "Q[1] = 0.6894846208805545\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.68948462]\n",
      "Action probabilities:\n",
      "no go:1.617942614906868e-05 ---- go:0.9999838205738509\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6778095780054504\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8389047890027252\n",
      "Learning rule:\n",
      "Q[1] <- 0.6894846208805545 + 0.2 * (0.8389047890027252 - 0.6894846208805545)\n",
      "Q[1] = 0.7193686545049887\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.71936865]\n",
      "Action probabilities:\n",
      "no go:1.0030215182368825e-05 ---- go:0.9999899697848176\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6703200460356393\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8351600230178197\n",
      "Learning rule:\n",
      "Q[1] <- 0.7193686545049887 + 0.2 * (0.8351600230178197 - 0.7193686545049887)\n",
      "Q[1] = 0.7425269282075548\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.74252693]\n",
      "Action probabilities:\n",
      "no go:6.924548019986679e-06 ---- go:0.9999930754519801\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6642538428642953\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8321269214321476\n",
      "Learning rule:\n",
      "Q[1] <- 0.7425269282075548 + 0.2 * (0.8321269214321476 - 0.7425269282075548)\n",
      "Q[1] = 0.7604469268524734\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (1, 1): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.6065306597126334\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (1, 1) is: -0.1967346701436833\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.2 * (-0.1967346701436833 - 0.0)\n",
      "Q[1] = -0.03934693402873666\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.         -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.6523876332412172 ---- go:0.3476123667587828\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.42324086244530706\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.42324086244530706 - 0.0)\n",
      "Q[0] = 0.08464817248906142\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [ 0.08464817 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.8790985975914353 ---- go:0.12090140240856483\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[0] <- 0.08464817248906142 + 0.2 * (0.5 - 0.08464817248906142)\n",
      "Q[0] = 0.16771853799124914\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.16771854 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9648737162012835 ---- go:0.03512628379871649\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.1051709180756475\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.5525854590378237\n",
      "Learning rule:\n",
      "Q[0] <- 0.16771853799124914 + 0.2 * (0.5525854590378237 - 0.16771853799124914)\n",
      "Q[0] = 0.24469192220056407\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.24469192 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9894874957627268 ---- go:0.010512504237273283\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.181360412865646\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.590680206432823\n",
      "Learning rule:\n",
      "Q[0] <- 0.24469192220056407 + 0.2 * (0.590680206432823 - 0.24469192220056407)\n",
      "Q[0] = 0.31388957904701587\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.31388958 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9965010393284215 ---- go:0.003498960671578459\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.238976597541378\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.619488298770689\n",
      "Learning rule:\n",
      "Q[0] <- 0.31388957904701587 + 0.2 * (0.619488298770689 - 0.31388957904701587)\n",
      "Q[0] = 0.3750093229917505\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.37500932 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9986811825303157 ---- go:0.0013188174696844486\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.2840254166877414\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6420127083438707\n",
      "Learning rule:\n",
      "Q[0] <- 0.3750093229917505 + 0.2 * (0.6420127083438707 - 0.3750093229917505)\n",
      "Q[0] = 0.42841000006217456\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.42841    -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.999438373056547 ---- go:0.000561626943453001\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.3201927884341202\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6600963942170601\n",
      "Learning rule:\n",
      "Q[0] <- 0.42841000006217456 + 0.2 * (0.6600963942170601 - 0.42841000006217456)\n",
      "Q[0] = 0.47474727889315166\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.47474728 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9997323352002196 ---- go:0.00026766479978035433\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.3498588075760032\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6749294037880016\n",
      "Learning rule:\n",
      "Q[0] <- 0.47474727889315166 + 0.2 * (0.6749294037880016 - 0.47474727889315166)\n",
      "Q[0] = 0.5147837038721217\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.5147837  -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.9998589267486854 ---- go:0.00014107325131458283\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.3746261705388516\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6873130852694258\n",
      "Learning rule:\n",
      "Q[0] <- 0.5147837038721217 + 0.2 * (0.6873130852694258 - 0.5147837038721217)\n",
      "Q[0] = 0.5492895801515825\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.2 * (0.8243606353500641 - 0.0)\n",
      "Q[0] = 0.16487212707001284\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0.16487213 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9332646509438911 ---- go:0.06673534905610894\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.16487212707001284 + 0.2 * (0.8243606353500641 - 0.16487212707001284)\n",
      "Q[0] = 0.2967698287260231\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0.29676983 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9914081928701108 ---- go:0.008591807129889115\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.2967698287260231 + 0.2 * (0.8243606353500641 - 0.2967698287260231)\n",
      "Q[0] = 0.40228799005083127\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0.40228799 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9984007306181839 ---- go:0.0015992693818160644\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.40228799005083127 + 0.2 * (0.8243606353500641 - 0.40228799005083127)\n",
      "Q[0] = 0.48670251911067786\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0.48670252 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9995851755819314 ---- go:0.0004148244180685738\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.48670251911067786 + 0.2 * (0.8243606353500641 - 0.48670251911067786)\n",
      "Q[0] = 0.5542341423585551\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0.55423414 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9998591601498296 ---- go:0.0001408398501704748\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.5542341423585551 + 0.2 * (0.8243606353500641 - 0.5542341423585551)\n",
      "Q[0] = 0.6082594409568569\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [0.60825944 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999406590182087 ---- go:5.934098179135035e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.6082594409568569 + 0.2 * (0.8243606353500641 - 0.6082594409568569)\n",
      "Q[0] = 0.6514796798354984\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.65147968 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999702804237826 ---- go:2.9719576217484e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.6514796798354984 + 0.2 * (0.8243606353500641 - 0.6514796798354984)\n",
      "Q[0] = 0.6860558709384115\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [0.68605587 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999829081877851 ---- go:1.709181221481057e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.6860558709384115 + 0.2 * (0.8243606353500641 - 0.6860558709384115)\n",
      "Q[0] = 0.713716823820742\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.71371682 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999890204945151 ---- go:1.0979505484950393e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[0] <- 0.713716823820742 + 0.2 * (0.8243606353500641 - 0.713716823820742)\n",
      "Q[0] = 0.7358455861266064\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16077b939d2f43f3807881249f837b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936977cae68a4b818e7d6b91e3579087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833ca0b68a4049ea93aa3bd7bb5799f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99848e6be25d46458fd372484206d714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c68452707242ea8dd0c3f70ceeafcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Fairness/M1/conditional_entropy_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Fairness/M1/entropy_bias.png\n",
      "Plotting efficiency...\n",
      "Plot saved to ../images/Fairness/M1/efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Fairness/M1/inequality_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['conditional_entropy', 'entropy', 'efficiency', 'inequality']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2 <a class=\"anchor\" id=\"m2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 1.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 1.0\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (1.0 - 0.0)\n",
      "Q[(0, 0), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.923240862445307\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (0.923240862445307 - 0.0)\n",
      "Q[(1, 0), 1] = 0.1846481724890614\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.18464817]\n",
      "Action probabilities:\n",
      "no go:0.04953034331652559 ---- go:0.9504696566834745\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7788007830714049\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8894003915357025\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.1846481724890614 + 0.2 * (0.8894003915357025 - 0.1846481724890614)\n",
      "Q[(1, 0), 1] = 0.3255986162983896\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.32559862]\n",
      "Action probabilities:\n",
      "no go:0.005434286927264131 ---- go:0.9945657130727359\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7408182206817179\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8704091103408589\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.3255986162983896 + 0.2 * (0.8704091103408589 - 0.3255986162983896)\n",
      "Q[(1, 0), 1] = 0.43456071510688343\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.43456072]\n",
      "Action probabilities:\n",
      "no go:0.0009548781893037806 ---- go:0.9990451218106963\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7165313105737893\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8582656552868946\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.43456071510688343 + 0.2 * (0.8582656552868946 - 0.43456071510688343)\n",
      "Q[(1, 0), 1] = 0.5193017031428857\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.5193017]\n",
      "Action probabilities:\n",
      "no go:0.00024627209582179333 ---- go:0.9997537279041782\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6996725373751304\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8498362686875651\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.5193017031428857 + 0.2 * (0.8498362686875651 - 0.5193017031428857)\n",
      "Q[(1, 0), 1] = 0.5854086162518216\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.58540862]\n",
      "Action probabilities:\n",
      "no go:8.553170813465591e-05 ---- go:0.9999144682918654\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6872892787909722\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8436446393954862\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.5854086162518216 + 0.2 * (0.8436446393954862 - 0.5854086162518216)\n",
      "Q[(1, 0), 1] = 0.6370558208805546\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.63705582]\n",
      "Action probabilities:\n",
      "no go:3.7434022714782584e-05 ---- go:0.9999625659772852\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6778095780054504\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8389047890027252\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.6370558208805546 + 0.2 * (0.8389047890027252 - 0.6370558208805546)\n",
      "Q[(1, 0), 1] = 0.6774256145049887\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.67742561]\n",
      "Action probabilities:\n",
      "no go:1.9622583122851583e-05 ---- go:0.9999803774168772\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6703200460356393\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8351600230178197\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.6774256145049887 + 0.2 * (0.8351600230178197 - 0.6774256145049887)\n",
      "Q[(1, 0), 1] = 0.7089724962075549\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.7089725]\n",
      "Action probabilities:\n",
      "no go:1.1845390128279133e-05 ---- go:0.9999881546098717\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.6642538428642953\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 0] is: 0.8321269214321476\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.7089724962075549 + 0.2 * (0.8321269214321476 - 0.7089724962075549)\n",
      "Q[(1, 0), 1] = 0.7336033812524734\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (1, 1): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.6065306597126334\n",
      "Payoff: -1\n",
      "G observed for action 1 in state (1, 1) is: -0.1967346701436833\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (-0.1967346701436833 - 0.0)\n",
      "Q[(1, 1), 1] = -0.03934693402873666\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.         -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.6523876332412172 ---- go:0.3476123667587828\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.42324086244530706\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.0 + 0.2 * (0.42324086244530706 - 0.0)\n",
      "Q[(1, 1), 0] = 0.08464817248906142\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.7788007830714049\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.11059960846429756\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (-0.11059960846429756 - 0.0)\n",
      "Q[(0, 1), 1] = -0.022119921692859514\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [ 0.08464817 -0.03934693]\n",
      "Action probabilities:\n",
      "no go:0.8790985975914353 ---- go:0.12090140240856483\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.7408182206817179\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 1] is: -0.12959088965914106\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- -0.03934693402873666 + 0.2 * (-0.12959088965914106 - -0.03934693402873666)\n",
      "Q[(1, 1), 1] = -0.05739572515481754\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.08464817 -0.05739573]\n",
      "Action probabilities:\n",
      "no go:0.9065908857937822 ---- go:0.09340911420621785\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.42324086244530706\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.08464817248906142 + 0.2 * (0.42324086244530706 - 0.08464817248906142)\n",
      "Q[(1, 1), 0] = 0.15236671048031056\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.02211992]\n",
      "Action probabilities:\n",
      "no go:0.5875675412137286 ---- go:0.4124324587862715\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.8071177470053893\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.09644112649730535\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- -0.022119921692859514 + 0.2 * (-0.09644112649730535 - -0.022119921692859514)\n",
      "Q[(0, 1), 1] = -0.03698416265374868\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 1]: [ 0.15236671 -0.05739573]\n",
      "Action probabilities:\n",
      "no go:0.9663072435868671 ---- go:0.03369275641313288\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.8824969025845955\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.4412484512922977\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.15236671048031056 + 0.2 * (0.4412484512922977 - 0.15236671048031056)\n",
      "Q[(1, 1), 0] = 0.210143058642708\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.03698416]\n",
      "Action probabilities:\n",
      "no go:0.6437657969765045 ---- go:0.35623420302349545\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 1] is: -0.07675913755469294\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- -0.03698416265374868 + 0.2 * (-0.07675913755469294 - -0.03698416265374868)\n",
      "Q[(0, 1), 1] = -0.044939157633937535\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 1]: [ 0.21014306 -0.05739573]\n",
      "Action probabilities:\n",
      "no go:0.9863546953503244 ---- go:0.013645304649675715\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9048374180359596\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.4524187090179798\n",
      "Learning rule:\n",
      "Q[(1, 1), 0] <- 0.210143058642708 + 0.2 * (0.4524187090179798 - 0.210143058642708)\n",
      "Q[(1, 1), 0] = 0.25859818871776236\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.04493916]\n",
      "Action probabilities:\n",
      "no go:0.6723926145978848 ---- go:0.32760738540211515\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9555630362682843\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.47778151813414216\n",
      "Learning rule:\n",
      "Q[(0, 1), 0] <- 0.0 + 0.2 * (0.47778151813414216 - 0.0)\n",
      "Q[(0, 1), 0] = 0.09555630362682843\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 1.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 1.0\n",
      "Learning rule:\n",
      "Q[(0, 0), 1] <- 0.0 + 0.2 * (1.0 - 0.0)\n",
      "Q[(0, 0), 1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.07675913755469294\n",
      "Learning rule:\n",
      "Q[(1, 0), 1] <- 0.0 + 0.2 * (-0.07675913755469294 - 0.0)\n",
      "Q[(1, 0), 1] = -0.015351827510938588\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.7788007830714049\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [1, 1] is: 0.8894003915357025\n",
      "Learning rule:\n",
      "Q[(1, 1), 1] <- 0.0 + 0.2 * (0.8894003915357025 - 0.0)\n",
      "Q[(1, 1), 1] = 0.1778800783071405\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [ 0.         -0.01535183]\n",
      "Action probabilities:\n",
      "no go:0.5611004172035499 ---- go:0.43889958279645025\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9048374180359596\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.4524187090179798\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.0 + 0.2 * (0.4524187090179798 - 0.0)\n",
      "Q[(1, 0), 0] = 0.09048374180359597\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.923240862445307\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.0 + 0.2 * (0.923240862445307 - 0.0)\n",
      "Q[(0, 1), 1] = 0.1846481724890614\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [ 0.09048374 -0.01535183]\n",
      "Action probabilities:\n",
      "no go:0.8446667168232864 ---- go:0.15533328317671363\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9310627797040228\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.4655313898520114\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.09048374180359597 + 0.2 * (0.4655313898520114 - 0.09048374180359597)\n",
      "Q[(1, 0), 0] = 0.16549327141327907\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.18464817]\n",
      "Action probabilities:\n",
      "no go:0.04953034331652559 ---- go:0.9504696566834745\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.8824969025845955\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.9412484512922977\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.1846481724890614 + 0.2 * (0.9412484512922977 - 0.1846481724890614)\n",
      "Q[(0, 1), 1] = 0.3359682282497087\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [ 0.16549327 -0.01535183]\n",
      "Action probabilities:\n",
      "no go:0.9475252549040662 ---- go:0.052474745095933695\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9459594689067654\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.4729797344533827\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.16549327141327907 + 0.2 * (0.4729797344533827 - 0.16549327141327907)\n",
      "Q[(1, 0), 0] = 0.2269905640212998\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [0.         0.33596823]\n",
      "Action probabilities:\n",
      "no go:0.004607316870060567 ---- go:0.9953926831299396\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 0.9048374180359596\n",
      "Payoff: 1\n",
      "G observed for action 1 in state [0, 1] is: 0.9524187090179799\n",
      "Learning rule:\n",
      "Q[(0, 1), 1] <- 0.3359682282497087 + 0.2 * (0.9524187090179799 - 0.3359682282497087)\n",
      "Q[(0, 1), 1] = 0.4592583244033629\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [ 0.22699056 -0.01535183]\n",
      "Action probabilities:\n",
      "no go:0.9797169524440617 ---- go:0.020283047555938304\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 0.9555630362682843\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 0] is: 0.47778151813414216\n",
      "Learning rule:\n",
      "Q[(1, 0), 0] <- 0.2269905640212998 + 0.2 * (0.47778151813414216 - 0.2269905640212998)\n",
      "Q[(1, 0), 0] = 0.27714875484386825\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':100,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9884e53f122d4bccb8df296f0edf6829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f64785850054b48ae1829964773c9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b462dbe3f8c94b8e93477ded3d27b772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac340e7e0335466391d03f2ab00b60ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76c01175684c9cbf84a8808c05a4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/Fairness/M2/efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Fairness/M2/inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Fairness/M2/entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Fairness/M2/conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3 <a class=\"anchor\" id=\"m3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (0, 0) is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[(0, 0),0] <- 0.0 + 0.2 * (0.8243606353500641 - 0.0)\n",
      "Q[(0, 0),0] = 0.16487212707001284\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0.16487213 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9332646509438911 ---- go:0.06673534905610894\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.16487212707001284 + 0.2 * (0.8243606353500641 - 0.16487212707001284)\n",
      "Q[[0, 0],0] = 0.2967698287260231\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0.29676983 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9914081928701108 ---- go:0.008591807129889115\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.2967698287260231 + 0.2 * (0.8243606353500641 - 0.2967698287260231)\n",
      "Q[[0, 0],0] = 0.40228799005083127\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0.40228799 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9984007306181839 ---- go:0.0015992693818160644\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.40228799005083127 + 0.2 * (0.8243606353500641 - 0.40228799005083127)\n",
      "Q[[0, 0],0] = 0.48670251911067786\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [0.48670252 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9995851755819314 ---- go:0.0004148244180685738\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.48670251911067786 + 0.2 * (0.8243606353500641 - 0.48670251911067786)\n",
      "Q[[0, 0],0] = 0.5542341423585551\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0.55423414 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9998591601498296 ---- go:0.0001408398501704748\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.5542341423585551 + 0.2 * (0.8243606353500641 - 0.5542341423585551)\n",
      "Q[[0, 0],0] = 0.6082594409568569\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [0.60825944 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999406590182087 ---- go:5.934098179135035e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.6082594409568569 + 0.2 * (0.8243606353500641 - 0.6082594409568569)\n",
      "Q[[0, 0],0] = 0.6514796798354984\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.65147968 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999702804237826 ---- go:2.9719576217484e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.6514796798354984 + 0.2 * (0.8243606353500641 - 0.6514796798354984)\n",
      "Q[[0, 0],0] = 0.6860558709384115\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [0.68605587 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999829081877851 ---- go:1.709181221481057e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.6860558709384115 + 0.2 * (0.8243606353500641 - 0.6860558709384115)\n",
      "Q[[0, 0],0] = 0.713716823820742\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.71371682 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9999890204945151 ---- go:1.0979505484950393e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.6487212707001282\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.8243606353500641\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.713716823820742 + 0.2 * (0.8243606353500641 - 0.713716823820742)\n",
      "Q[[0, 0],0] = 0.7358455861266064\n"
     ]
    }
   ],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (1, 1): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state (1, 1) is: 0.5\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[(1, 1),0] = 0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.181360412865646\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.590680206432823\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0.590680206432823 - 0.0)\n",
      "Q[[0, 1],0] = 0.11813604128656462\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0.11813604 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.8687756719759302 ---- go:0.1312243280240698\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.2840254166877414\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6420127083438707\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.11813604128656462 + 0.2 * (0.6420127083438707 - 0.11813604128656462)\n",
      "Q[[0, 1],0] = 0.22291137469802585\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0.22291137 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9725240036786552 ---- go:0.027475996321344766\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.3498588075760032\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6749294037880016\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.22291137469802585 + 0.2 * (0.6749294037880016 - 0.22291137469802585)\n",
      "Q[[0, 1],0] = 0.313314980516021\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [0.31331498 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9933932823831011 ---- go:0.006606717616898933\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.3956124250860895\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.6978062125430448\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.313314980516021 + 0.2 * (0.6978062125430448 - 0.313314980516021)\n",
      "Q[[0, 1],0] = 0.3902132269214258\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.39021323 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9980605540894368 ---- go:0.0019394459105631947\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.4292400324179777\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.7146200162089889\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.3902132269214258 + 0.2 * (0.7146200162089889 - 0.3902132269214258)\n",
      "Q[[0, 1],0] = 0.4550945847789384\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [0.45509458 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9993123298465856 ---- go:0.000687670153414473\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.4549914146182013\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.7274957073091006\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.4550945847789384 + 0.2 * (0.7274957073091006 - 0.4550945847789384)\n",
      "Q[[0, 1],0] = 0.5095748092849708\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.50957481 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9997122690514545 ---- go:0.00028773094854544075\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.4753406154906223\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.7376703077453112\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.5095748092849708 + 0.2 * (0.7376703077453112 - 0.5095748092849708)\n",
      "Q[[0, 1],0] = 0.5551939089770389\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [0.55519391 0.        ]\n",
      "Action probabilities:\n",
      "no go:0.9998613061049547 ---- go:0.00013869389504538488\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.4918246976412703\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.7459123488206352\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.5551939089770389 + 0.2 * (0.7459123488206352 - 0.5551939089770389)\n",
      "Q[[0, 1],0] = 0.5933375969457582\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.5933376 0.       ]\n",
      "Action probabilities:\n",
      "no go:0.9999246583078417 ---- go:7.534169215824796e-05\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.5054485732260887\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.7527242866130444\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.5933375969457582 + 0.2 * (0.7527242866130444 - 0.5933375969457582)\n",
      "Q[[0, 1],0] = 0.6252149348792154\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state (0, 0): [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average fairness: 1.0\n",
      "Payoff: 1\n",
      "G observed for action 1 in state (0, 0) is: 1.0\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.0 + 0.2 * (1.0 - 0.0)\n",
      "Q[(0, 0),1] = 0.2\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.8464817248906141\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [1, 0] is: -0.07675913755469294\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.0 + 0.2 * (-0.07675913755469294 - 0.0)\n",
      "Q[[1, 0],1] = -0.015351827510938588\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.2 * (0.5 - 0.0)\n",
      "Q[[1, 1],0] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0.  0.2]\n",
      "Action probabilities:\n",
      "no go:0.039165722796764356 ---- go:0.9608342772032356\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 0.9048374180359596\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 0] is: -0.047581290982020186\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.2 + 0.2 * (-0.047581290982020186 - 0.2)\n",
      "Q[[0, 0],1] = 0.15048374180359597\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [0.1 0. ]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339246 ---- go:0.16798161486607552\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.0\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.5\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.1 + 0.2 * (0.5 - 0.1)\n",
      "Q[[1, 1],0] = 0.18000000000000002\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0.         0.15048374]\n",
      "Action probabilities:\n",
      "no go:0.08258439371417088 ---- go:0.9174156062858292\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.074041430716296\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 0] is: 0.537020715358148\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.2 * (0.537020715358148 - 0.0)\n",
      "Q[[0, 0],0] = 0.1074041430716296\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.1331484530668263\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [0, 1] is: 0.5665742265334132\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.2 * (0.5665742265334132 - 0.0)\n",
      "Q[[0, 1],0] = 0.11331484530668263\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.10740414 0.15048374]\n",
      "Action probabilities:\n",
      "no go:0.33419468853588863 ---- go:0.6658053114641114\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 1.0571277447602365\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 0] is: 0.02856387238011826\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.15048374180359597 + 0.2 * (0.02856387238011826 - 0.15048374180359597)\n",
      "Q[[0, 0],1] = 0.12609976791890043\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 1]: [0.18 0.  ]\n",
      "Action probabilities:\n",
      "no go:0.9468488636019363 ---- go:0.05315113639806371\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average fairness: 1.1051709180756475\n",
      "Payoff: 0\n",
      "G observed for action 0 in state [1, 1] is: 0.5525854590378237\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.18000000000000002 + 0.2 * (0.5525854590378237 - 0.18000000000000002)\n",
      "Q[[1, 1],0] = 0.25451709180756477\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.10740414 0.12609977]\n",
      "Action probabilities:\n",
      "no go:0.42577017627857544 ---- go:0.5742298237214246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average fairness: 1.0465034351948703\n",
      "Payoff: -1\n",
      "G observed for action 1 in state [0, 0] is: 0.023251717597435162\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.12609976791890043 + 0.2 * (0.023251717597435162 - 0.12609976791890043)\n",
      "Q[[0, 0],1] = 0.10553015785460737\n"
     ]
    }
   ],
   "source": [
    "agent = FairnessM3(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.5,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb9ef21ee4b4447adcfedf8c7fe1f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each bias:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35555e021b7d4e209296efe493aa79df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f45c2202204686afc0843d4be05ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42db24ca9c524fff9ef6e07543ba5f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c2f321266c49ea8ec2235205689d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../images/Fairness/M3/efficiency_bias.png\n",
      "Plotting inequality...\n",
      "Plot saved to ../images/Fairness/M3/inequality_bias.png\n",
      "Plotting entropy...\n",
      "Plot saved to ../images/Fairness/M3/entropy_bias.png\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../images/Fairness/M3/conditional_entropy_bias.png\n"
     ]
    }
   ],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=FairnessM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='bias',\n",
    "    values=[0, 0.2, 0.4, 0.6],\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}\n",
    "\n",
    "list_dicts = [\n",
    "    {\n",
    "        'agent_class': FairnessM1,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 7\n",
    "    },\n",
    "    {\n",
    "        'agent_class': FairnessM2,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 9\n",
    "    },\n",
    "    {\n",
    "        'agent_class': FairnessM3,\n",
    "        'fixed_parameters': fixed_parameters,\n",
    "        'free_parameters': free_parameters,\n",
    "        'simulation_parameters': simulation_parameters,\n",
    "        'seed': 0\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e125eaf172b4a2482a680404a73e4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251c472980964642908bc337dfd7ac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66207a8799541bfb071fc96eac0dc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#-------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Create plots\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#-------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m perf = \u001b[43mPerformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimple_vs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlist_dicts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_folder_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mefficiency\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minequality\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mentropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconditional_entropy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mT\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_names\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAttendance-M1-7\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mM1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAttendance-M2-9\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mM2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAttendance-M3-0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mM3\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfigsize\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorios/Cognitive_Models_El_Farol_Bar_Problem/Notebooks/../src/Utils/interaction.py:803\u001b[39m, in \u001b[36mPerformer.simple_vs\u001b[39m\u001b[34m(list_dicts, image_folder, measures, kwargs)\u001b[39m\n\u001b[32m    801\u001b[39m     p = PlotStandardMeasures(df)\n\u001b[32m    802\u001b[39m     categorical = kwargs.get(\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m803\u001b[39m     list_images = \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot_measures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m\t\t\t\t\t\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstandard_measures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcategorical\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcategorical\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m#-------------------------------\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# Create latex string\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m#-------------------------------\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;66;03m# Presenting fixed parameters\u001b[39;00m\n\u001b[32m    813\u001b[39m latex_string = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mnoindent\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtextbf\u001b[39m\u001b[33m{\u001b[39m\u001b[33mFixed parameters:}\u001b[39m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorios/Cognitive_Models_El_Farol_Bar_Problem/Notebooks/../src/Utils/plot_utils.py:114\u001b[39m, in \u001b[36mPlotStandardMeasures.plot_measures\u001b[39m\u001b[34m(self, measures, folder, kwargs, categorical, suffix)\u001b[39m\n\u001b[32m    112\u001b[39m kwargs[\u001b[33m'\u001b[39m\u001b[33mvs_models\u001b[39m\u001b[33m'\u001b[39m] = vs_models\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Obtain data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#Initialize output list\u001b[39;00m\n\u001b[32m    116\u001b[39m list_of_paths = \u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorios/Cognitive_Models_El_Farol_Bar_Problem/Notebooks/../src/Utils/plot_utils.py:288\u001b[39m, in \u001b[36mPlotStandardMeasures.get_data\u001b[39m\u001b[34m(self, measures, T)\u001b[39m\n\u001b[32m    286\u001b[39m ai_dict = AlternationIndex.check_alternation_index_in_measures(measures)\n\u001b[32m    287\u001b[39m \u001b[38;5;66;03m# Get other measures\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m get_meas = \u001b[43mGetMeasurements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmeasures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mai_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmeasures\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m data = get_meas.get_measurements()\n\u001b[32m    294\u001b[39m ordered_models = OrderStrings.dict_as_numeric(data[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m].unique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorios/Cognitive_Models_El_Farol_Bar_Problem/Notebooks/../src/Utils/utils.py:523\u001b[39m, in \u001b[36mGetMeasurements.__init__\u001b[39m\u001b[34m(self, data, measures, normalize, T, per_round, per_player)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;28mself\u001b[39m.columns = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data.columns]\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m#-----------------------------\u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m#Keep only T last rounds\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m#-----------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkeep_last_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorios/Cognitive_Models_El_Farol_Bar_Problem/Notebooks/../src/Utils/utils.py:589\u001b[39m, in \u001b[36mGetMeasurements.keep_last_rounds\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    587\u001b[39m     df = grp[grp[\u001b[33m\"\u001b[39m\u001b[33mround\u001b[39m\u001b[33m\"\u001b[39m] >= (num_rounds - \u001b[38;5;28mself\u001b[39m.T)].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    588\u001b[39m     df_list.append(df)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RLenv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RLenv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/RLenv/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "#-------------------------------\n",
    "# Create plots\n",
    "#-------------------------------\n",
    "perf = Performer.simple_vs(\n",
    "    list_dicts=list_dicts,\n",
    "    image_folder=image_folder_all,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "    kwargs={\n",
    "        'T': 20,\n",
    "        'model_names': {\n",
    "            'Attendance-M1-7': 'M1',\n",
    "            'Attendance-M2-9': 'M2',\n",
    "            'Attendance-M3-0': 'M3'\n",
    "        },\n",
    "        'figsize': (3.5, 3)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Performer\n",
    "\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":6,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.2,\n",
    "\t\"bias\": 0.3,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':1,\n",
    "\t'num_rounds':1000,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M1,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M2,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LaTeX_string = Performer.simple_run(\n",
    "    agent_class=FairnessM3,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    image_folder=image_folder_M3,\n",
    "    measures=['render']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
