{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import PayoffM1, PayoffM2, PayoffM3\n",
    "from Utils.unit_tests import (\n",
    "    test_bar_is_full, \n",
    "    test_bar_has_capacity,\n",
    "    test_alternation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"inverse_temperature\":32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.1 * (1 - 0.0)\n",
      "Q[1] = 0.1\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.16798161486607552 ---- go:0.8320183851339246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.1 + 0.1 * (1 - 0.1)\n",
      "Q[1] = 0.19\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.19]\n",
      "Action probabilities:\n",
      "no go:0.04565117078444372 ---- go:0.9543488292155563\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.19 + 0.1 * (1 - 0.19)\n",
      "Q[1] = 0.271\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.    0.271]\n",
      "Action probabilities:\n",
      "no go:0.01291967596744309 ---- go:0.9870803240325569\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.271 + 0.1 * (1 - 0.271)\n",
      "Q[1] = 0.34390000000000004\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.3439]\n",
      "Action probabilities:\n",
      "no go:0.004060420713976014 ---- go:0.995939579286024\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.34390000000000004 + 0.1 * (1 - 0.34390000000000004)\n",
      "Q[1] = 0.40951000000000004\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.      0.40951]\n",
      "Action probabilities:\n",
      "no go:0.001424996369986182 ---- go:0.9985750036300138\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.40951000000000004 + 0.1 * (1 - 0.40951000000000004)\n",
      "Q[1] = 0.46855900000000006\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.468559]\n",
      "Action probabilities:\n",
      "no go:0.0005544695742134787 ---- go:0.9994455304257864\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.46855900000000006 + 0.1 * (1 - 0.46855900000000006)\n",
      "Q[1] = 0.5217031000000001\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.        0.5217031]\n",
      "Action probabilities:\n",
      "no go:0.00023699142012064984 ---- go:0.9997630085798794\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [1, 0] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.5217031000000001 + 0.1 * (1 - 0.5217031000000001)\n",
      "Q[1] = 0.5695327900000001\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (1, 1) is: -1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.1 * (-1 - 0.0)\n",
      "Q[1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.8320183851339245 ---- go:0.1679816148660755\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[1] <- -0.1 + 0.1 * (-1 - -0.1)\n",
      "Q[1] = -0.19\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[1] <- 0.0 + 0.1 * (1 - 0.0)\n",
      "Q[1] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.16798161486607552 ---- go:0.8320183851339246\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [1, 0] is: -1\n",
      "Learning rule:\n",
      "Q[1] <- 0.1 + 0.1 * (-1 - 0.1)\n",
      "Q[1] = -0.010000000000000009\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [ 0.   -0.01]\n",
      "Action probabilities:\n",
      "no go:0.5399148845555657 ---- go:0.46008511544443426\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 0]: [ 0.   -0.01]\n",
      "Action probabilities:\n",
      "no go:0.5399148845555657 ---- go:0.46008511544443426\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.   -0.01]\n",
      "Action probabilities:\n",
      "no go:0.5399148845555657 ---- go:0.46008511544443426\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 0]: [ 0.   -0.01]\n",
      "Action probabilities:\n",
      "no go:0.5399148845555657 ---- go:0.46008511544443426\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 0] is: -1\n",
      "Learning rule:\n",
      "Q[1] <- -0.010000000000000009 + 0.1 * (-1 - -0.010000000000000009)\n",
      "Q[1] = -0.10900000000000001\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 1]: [ 0.    -0.109]\n",
      "Action probabilities:\n",
      "no go:0.8511944274200296 ---- go:0.14880557257997035\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 0]: [ 0.    -0.109]\n",
      "Action probabilities:\n",
      "no go:0.8511944274200296 ---- go:0.14880557257997035\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.    -0.109]\n",
      "Action probabilities:\n",
      "no go:0.8511944274200296 ---- go:0.14880557257997035\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM1(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../../images/Payoff/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../../data/Payoff/M1')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':100,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdbb86a4af148bd99701128633a2b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bc14e4bf574d7a928165e5a4eecaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f09b46cf844302ace04359973f287a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3609795a0e46eeae8372d986f8e63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953cb5fec9f84a2098427392c9a7b9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a275db94b845fc8ae475b59a553974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d035f4d4a35476a839cdd209344d640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6661977ad9584c77ade0ca31863103e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M1/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/Payoff/M1/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M1/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/Payoff/M1/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f372b99c3524eefa53489de60663a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each inverse_temperature:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da82c041b03f41b9bff8a8e2785dd18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1406f4ef8ee941fc8806fb1f480d6512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc0c30a1b9b4c82ab0ee371406760ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11078005b3fc4b46b47dfd94a6a46f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f89e78a1ad542e48f68999f6f13a0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M1/entropy_inverse_temperature.pdf\n",
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M1/efficiency_inverse_temperature.pdf\n"
     ]
    }
   ],
   "source": [
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='inverse_temperature',\n",
    "    values=[2**x for x in range(2, 7)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['entropy', 'efficiency']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX_string = Performer.simple_plots(\n",
    "\tagent_class=QAttendance,\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tsimulation_parameters=simulation_parameters,\n",
    "\tmeasures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "\timage_folder=image_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fa9b548cfc4d3ead4369217af26c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1b29054a3c46cc936aa4742d8dbedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d5e77b4a9f4951945833792f0b2c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1173aa12113b4aaeabb94085d185a41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a053866751041419ceaeced12a23b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db397a6e06e840a08b2db5d8d700315c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e990fe9fffde485da25e8991b0197a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b67f29847846b7a2692efe8fb6d51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b425bbd23ef94ab0a2e5db7b46d9a1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca34778429f4c23a340d23a906a50ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccef0a359bc473381b3da5f982a2336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1346e86428744123b40f00d79df62d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a41bcc2657948d5a3ee00187bb2cab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bccff56856c4abea45285f2138ec7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d005c36af404a7d875fdbfabf53e863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca589a1c9ab4aa7800d2b2f3cd2cc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4842cba8ec554d9f9d21827b591b31dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9b5e68b2c44802a41221ef24362076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580f46676c694282aa0ac14cfbaffec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822fb60a6b6b4d898cb8f8b63aa58ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885d46f4ce5249f6bd77202d0e645ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe36a361bcb44f6b175dc75b643fa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1d73ac71c446899a002c07665e1eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf13bdffb0b43be9a829bd0253351e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176931acd9b24af19f88c22cbcca0cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104304f1c1104e3f9e49456ac5c8d0fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e8860e62c7432f88dbf5a3db85404e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6437c6da2c7a42f697574878288b642b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc77119024c4e7aad388ad2b3634cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5f88aea1d641d4b031787a6b22c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eb16913e534e17817ca94266ef8549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74c7ba8c64f4c46bc9046936b07e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195aef8bc578434e8aea90f2dcd17e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a9e71f7ed84a6cbbe9208c1dab6b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f835982817c4ee4bf81db995745b280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e6f251f1954f369751b6f93d894167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(2, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M1/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M1/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state (0, 0) is: 0\n",
      "Learning rule:\n",
      "Q[0, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 0, 0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.0 + 0.1 * (1 - 0.0)\n",
      "Q[0, 0, 1] = 0.1\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.03916572279676435 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.1 + 0.1 * (1 - 0.1)\n",
      "Q[0, 0, 1] = 0.19\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0.   0.19]\n",
      "Action probabilities:\n",
      "no go:0.0022829528535030625 ---- go:0.9977170471464969\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.19 + 0.1 * (1 - 0.19)\n",
      "Q[0, 0, 1] = 0.271\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.    0.271]\n",
      "Action probabilities:\n",
      "no go:0.0001712867849165666 ---- go:0.9998287132150835\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.271 + 0.1 * (1 - 0.271)\n",
      "Q[0, 0, 1] = 0.34390000000000004\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.     0.3439]\n",
      "Action probabilities:\n",
      "no go:1.6621448445428316e-05 ---- go:0.9999833785515546\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 0] is: 1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.34390000000000004 + 0.1 * (1 - 0.34390000000000004)\n",
      "Q[0, 0, 1] = 0.40951000000000004\n"
     ]
    }
   ],
   "source": [
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"inverse_temperature\":32\n",
    "}\n",
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_has_capacity(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (1, 1) is: -1\n",
      "Learning rule:\n",
      "Q[1, 1, 1] <- 0.0 + 0.1 * (-1 - 0.0)\n",
      "Q[1, 1, 1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[1, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 1, 0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state [0, 1] is: -1\n",
      "Learning rule:\n",
      "Q[0, 1, 1] <- 0.0 + 0.1 * (-1 - 0.0)\n",
      "Q[0, 1, 1] = -0.1\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[1, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 1, 0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 1] is: 0\n",
      "Learning rule:\n",
      "Q[0, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 1, 0] = 0.0\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_bar_is_full(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test other player alternates\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "G observed for action 1 in state (0, 0) is: -1\n",
      "Learning rule:\n",
      "Q[0, 0, 1] <- 0.0 + 0.1 * (-1 - 0.0)\n",
      "Q[0, 0, 1] = -0.1\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 1] is: 0\n",
      "Learning rule:\n",
      "Q[1, 1, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 1, 0] = 0.0\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [ 0.  -0.1]\n",
      "Action probabilities:\n",
      "no go:0.9608342772032357 ---- go:0.039165722796764356\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [0, 0] is: 0\n",
      "Learning rule:\n",
      "Q[0, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[0, 0, 0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[0, 1, 1] <- 0.0 + 0.1 * (1 - 0.0)\n",
      "Q[0, 1, 1] = 0.1\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [0.  0.1]\n",
      "Action probabilities:\n",
      "no go:0.03916572279676435 ---- go:0.9608342772032357\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[0, 1, 1] <- 0.1 + 0.1 * (1 - 0.1)\n",
      "Q[0, 1, 1] = 0.19\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [0.   0.19]\n",
      "Action probabilities:\n",
      "no go:0.0022829528535030625 ---- go:0.9977170471464969\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[0, 1, 1] <- 0.19 + 0.1 * (1 - 0.19)\n",
      "Q[0, 1, 1] = 0.271\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "G observed for action 0 in state [1, 0] is: 0\n",
      "Learning rule:\n",
      "Q[1, 0, 0] <- 0.0 + 0.1 * (0 - 0.0)\n",
      "Q[1, 0, 0] = 0.0\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [0.    0.271]\n",
      "Action probabilities:\n",
      "no go:0.0001712867849165666 ---- go:0.9998287132150835\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "G observed for action 1 in state [0, 1] is: 1\n",
      "Learning rule:\n",
      "Q[0, 1, 1] <- 0.271 + 0.1 * (1 - 0.271)\n",
      "Q[0, 1, 1] = 0.34390000000000004\n"
     ]
    }
   ],
   "source": [
    "agent = PayoffM2(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "test_alternation(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../../images/Payoff/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../../data/Payoff/M2')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"inverse_temperature\":16\n",
    "}\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':100,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d1278ff2cd46ff98fb3f18006c2722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each learning_rate:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85138daff5df4710aeb718229f720916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6503b136954b4a28a0efc53e176e73fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bc6946c32d4f9aba534af3e2f84339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8898cd6cf6ee4363a652031121030599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d5ef6d8d59416993060993b433304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c19cc0b4ca4d6f962bb9dedd2a71c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886b3fb4182f412b9ee7ad0b4008115d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../../images/Payoff/M2/efficiency_learning_rate.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/Payoff/M2/inequality_learning_rate.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/Payoff/M2/entropy_learning_rate.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/Payoff/M2/conditional_entropy_learning_rate.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=PayoffM2,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='learning_rate',\n",
    "    values=[0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=PayoffM1,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f3e406b5eb4e4f8b1e5593b441f521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d06b7ec816747c0bc21e65b59e1e029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337ac958c5354a8db407448233b3e24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306c6799435f4d5bacf01165bd04eb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de93f858cac4ceb9bbc52e6d2ae918a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700ab61177b64921ab7e32ab9e4f365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400b60fd8dc340ff8ff0e0a6d08636d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b920a2380bed4b1ea308b6e02ab140e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635d72a6c346453e8aa45a2168959f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8ce9b16a0e454e98e282914d4d141f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10facb27406743faba690d4b5ddb7bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da0b07fa3246baa1ab30998fc8b4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1d443c33404ca6ab8a4a61793ca188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17ffee0ff614f7fbdea955fd9989294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafcf812ed0a4e8b959b014be169cc98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22d3c244819468a88220a7652e409e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6890da09b174bfb84d2332014f4d0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6859ad392392450c94a306f715f203ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f1834f08fb4c639c522ef4a0edb4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f868c8114b548abb4f64c0b3b198d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc7dd9e381e45f2a967a274c78808fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59a6a769a1941d0bc6aecea3baea6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75665c85a3274f70867a14a87220ef0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c4c8ec2146445fba58ea012c0ee461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73126a2ee7f04ed1aafe76ac09bcbf27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac2302d2ba447a8adb384a7c361bab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2001a60a744c5d8dc96d55a34675aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7308245d933c4ac6be57fa6635d2ea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6300dc434cf84357aaa5c1058f0ca35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d75fa12250478987fc8e008c2937a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0136d8d12cdb495b826c0e3bb203d810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525f557f69ce4b1685f74d8982cf8dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d6e8c68efc4a85a7ed3684aea3b440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e1ab7acd974e3e9944bdc4c85b1b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "234789a0d55f4936983d57af83e9b020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336b2c26a37494491ebfea3340dafab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.run_sweep2(\n",
    "    parameter1='inverse_temperature',\n",
    "    values1=[2**x for x in range(2, 7)],\n",
    "    parameter2='learning_rate',\n",
    "    values2=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "    file=image_folder / 'sweep_inverse_temp_vs_learning_rate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotStandardMeasures\n",
    "\n",
    "p = PlotStandardMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M2/efficiency_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='efficiency',\n",
    "    file=image_folder / Path('efficiency_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to ../../images/Payoff/M2/entropy_sweep_inverse_temp_vs_learning_rate.png\n"
     ]
    }
   ],
   "source": [
    "p.plot_sweep2(\n",
    "    parameter1='inverse_temperature', \n",
    "    parameter2='learning_rate',\n",
    "    measure='entropy',\n",
    "    file=image_folder / Path('entropy_sweep_inverse_temp_vs_learning_rate.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
