{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import QAttendance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DASH_LINE = '-'*60\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"discount_factor\": 0.7,\n",
    "\t\"go_discount_factor\": 0.5,\n",
    "\t\"go_drive\":0.01,\n",
    "\t\"inverse_temperature\":32\n",
    "}\n",
    "agent = QAttendance(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0.   0.01]\n",
      "Action probabilities:\n",
      "no go:0.4206757478512505 ---- go:0.5793242521487494\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.25]\n",
      "Discounted average go frequency: 0.25\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.006999999999999999\n",
      "Reward with average go frequency: 0.257\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.1 * (0.257 - 0.0)\n",
      "Q[(1, 1),0] = 0.0257\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0.   0.01]\n",
      "Action probabilities:\n",
      "no go:0.4206757478512505 ---- go:0.5793242521487494\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.125]\n",
      "Discounted average go frequency: 0.125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.006999999999999999\n",
      "Reward with average go frequency: 0.132\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.1 * (0.132 - 0.0)\n",
      "Q[[0, 1],0] = 0.013200000000000002\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 1]: [0.0132 0.01  ]\n",
      "Action probabilities:\n",
      "no go:0.5255776538100527 ---- go:0.47442234618994733\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Discounted actions: [0.5, 0.0, 0.0, 0.0625]\n",
      "Discounted average go frequency: 0.5625\n",
      "Reward: -1\n",
      "Estimated long term reward: -0.98201\n",
      "Reward with average go frequency: -0.41951000000000005\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.01 + 0.1 * (-0.41951000000000005 - 0.01)\n",
      "Q[[0, 1],1] = -0.03295100000000001\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 1]: [0.0257 0.01  ]\n",
      "Action probabilities:\n",
      "no go:0.6230231741257082 ---- go:0.3769768258742919\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.25, 0.0, 0.0, 0.03125]\n",
      "Discounted average go frequency: 0.28125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.00924\n",
      "Reward with average go frequency: 0.29049\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0257 + 0.1 * (0.29049 - 0.0257)\n",
      "Q[[1, 1],0] = 0.052179\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [0, 1]: [ 0.0132   -0.032951]\n",
      "Action probabilities:\n",
      "no go:0.8140935960860921 ---- go:0.18590640391390795\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.125, 0.0, 0.0, 0.015625]\n",
      "Discounted average go frequency: 0.140625\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.00924\n",
      "Reward with average go frequency: 0.149865\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.013200000000000002 + 0.1 * (0.149865 - 0.013200000000000002)\n",
      "Q[[0, 1],0] = 0.0268665\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.0268665 -0.032951 ]\n",
      "Action probabilities:\n",
      "no go:0.8714857812946714 ---- go:0.1285142187053286\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0078125]\n",
      "Discounted average go frequency: 0.0703125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.01880655\n",
      "Reward with average go frequency: 0.08911905\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0268665 + 0.1 * (0.08911905 - 0.0268665)\n",
      "Q[[0, 1],0] = 0.033091755\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [ 0.03309176 -0.032951  ]\n",
      "Action probabilities:\n",
      "no go:0.8921957195468617 ---- go:0.1078042804531383\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.00390625]\n",
      "Discounted average go frequency: 0.03515625\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.023164228499999998\n",
      "Reward with average go frequency: 0.058320478499999995\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.033091755 + 0.1 * (0.058320478499999995 - 0.033091755)\n",
      "Q[[0, 1],0] = 0.03561462735\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [ 0.03561463 -0.032951  ]\n",
      "Action probabilities:\n",
      "no go:0.899718443146803 ---- go:0.10028155685319694\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.001953125]\n",
      "Discounted average go frequency: 0.017578125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.024930239144999998\n",
      "Reward with average go frequency: 0.042508364145\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.03561462735 + 0.1 * (0.042508364145 - 0.03561462735)\n",
      "Q[[0, 1],0] = 0.036304001029499995\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [ 0.036304 -0.032951]\n",
      "Action probabilities:\n",
      "no go:0.9016913300274012 ---- go:0.09830866997259877\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0, 0.0, 0.0009765625]\n",
      "Discounted average go frequency: 0.0087890625\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.025412800720649994\n",
      "Reward with average go frequency: 0.03420186322065\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.036304001029499995 + 0.1 * (0.03420186322065 - 0.036304001029499995)\n",
      "Q[[0, 1],0] = 0.03609378724861499\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [ 0.03609379 -0.032951  ]\n",
      "Action probabilities:\n",
      "no go:0.9010934220670939 ---- go:0.09890657793290618\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Discounted actions: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00390625, 0.0, 0.0, 0.00048828125]\n",
      "Discounted average go frequency: 0.00439453125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.025265651074030494\n",
      "Reward with average go frequency: 0.029660182324030494\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.03609378724861499 + 0.1 * (0.029660182324030494 - 0.03609378724861499)\n",
      "Q[[0, 1],0] = 0.03545042675615654\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(DASH_LINE)\n",
    "print('Test bar is full')\n",
    "print(DASH_LINE)    \n",
    "action = 1\n",
    "state = [action, 1]\n",
    "print('Initial state:', state)\n",
    "agent.decisions.append(action)\n",
    "agent.prev_state_ = tuple(state)\n",
    "for i in range(10):\n",
    "\tprint(f'---------- Round {i} ----------')\n",
    "\tpreferences = agent.determine_action_preferences(state)\n",
    "\tprint(f'Action preferences in state {state}: {preferences}')\n",
    "\taction = agent.make_decision()\n",
    "\tprint('Chosen action:', action)\n",
    "\tnew_state = [action, 1]\n",
    "\tprint('State arrived:', new_state)\n",
    "\tpayoff = agent.payoff(action, new_state)\n",
    "\tprint(f'Payoff action {action}: {payoff}')\n",
    "\tagent.update(payoff, new_state)\n",
    "\tstate = new_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [0, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [0, 0]: [0.   0.01]\n",
      "Action probabilities:\n",
      "no go:0.4206757478512505 ---- go:0.5793242521487494\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.0]\n",
      "Discounted average go frequency: 0.5\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.007\n",
      "Reward with average go frequency: 1.507\n",
      "Learning rule:\n",
      "Q[(0, 0),1] <- 0.01 + 0.1 * (1.507 - 0.01)\n",
      "Q[(0, 0),1] = 0.1597\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [0.   0.01]\n",
      "Action probabilities:\n",
      "no go:0.4206757478512505 ---- go:0.5793242521487494\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.0]\n",
      "Discounted average go frequency: 0.75\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.007\n",
      "Reward with average go frequency: 1.757\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.01 + 0.1 * (1.757 - 0.01)\n",
      "Q[[1, 0],1] = 0.1847\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 0]: [0.     0.1847]\n",
      "Action probabilities:\n",
      "no go:0.002703772027206484 ---- go:0.9972962279727935\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0]\n",
      "Discounted average go frequency: 0.875\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.12929\n",
      "Reward with average go frequency: 2.00429\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.1847 + 0.1 * (2.00429 - 0.1847)\n",
      "Q[[1, 0],1] = 0.366659\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [1, 0]: [0.       0.366659]\n",
      "Action probabilities:\n",
      "no go:8.023819329261877e-06 ---- go:0.9999919761806707\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.0]\n",
      "Discounted average go frequency: 0.9375\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.2566613\n",
      "Reward with average go frequency: 2.1941613\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.366659 + 0.1 * (2.1941613 - 0.366659)\n",
      "Q[[1, 0],1] = 0.54940923\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.54940923]\n",
      "Action probabilities:\n",
      "no go:2.31540671932094e-08 ---- go:0.9999999768459329\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.0]\n",
      "Discounted average go frequency: 0.96875\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.384586461\n",
      "Reward with average go frequency: 2.353336461\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.54940923 + 0.1 * (2.353336461 - 0.54940923)\n",
      "Q[[1, 0],1] = 0.7298019531\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.72980195]\n",
      "Action probabilities:\n",
      "no go:7.204987709800519e-11 ---- go:0.9999999999279502\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0]\n",
      "Discounted average go frequency: 0.984375\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.51086136717\n",
      "Reward with average go frequency: 2.49523636717\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.7298019531 + 0.1 * (2.49523636717 - 0.7298019531)\n",
      "Q[[1, 0],1] = 0.9063453945069999\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [0.         0.90634539]\n",
      "Action probabilities:\n",
      "no go:2.535912631545861e-13 ---- go:0.9999999999997464\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.0]\n",
      "Discounted average go frequency: 0.9921875\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.6344417761549\n",
      "Reward with average go frequency: 2.6266292761549\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 0.9063453945069999 + 0.1 * (2.6266292761549 - 0.9063453945069999)\n",
      "Q[[1, 0],1] = 1.07837378267179\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [1, 0]: [0.         1.07837378]\n",
      "Action probabilities:\n",
      "no go:1.031295057066762e-15 ---- go:0.999999999999999\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.0]\n",
      "Discounted average go frequency: 0.99609375\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.754861647870253\n",
      "Reward with average go frequency: 2.750955397870253\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 1.07837378267179 + 0.1 * (2.750955397870253 - 1.07837378267179)\n",
      "Q[[1, 0],1] = 1.2456319441916364\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [0.         1.24563194]\n",
      "Action probabilities:\n",
      "no go:4.885684799282504e-18 ---- go:1.0\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125, 0.0]\n",
      "Discounted average go frequency: 0.998046875\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.8719423609341455\n",
      "Reward with average go frequency: 2.8699892359341455\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 1.2456319441916364 + 0.1 * (2.8699892359341455 - 1.2456319441916364)\n",
      "Q[[1, 0],1] = 1.4080676733658872\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [1, 0]: [0.         1.40806767]\n",
      "Action probabilities:\n",
      "no go:2.7007683650478283e-20 ---- go:1.0\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Discounted actions: [0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625, 0.0078125, 0.00390625, 0.001953125, 0.0009765625, 0.0]\n",
      "Discounted average go frequency: 0.9990234375\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.9856473713561211\n",
      "Reward with average go frequency: 2.984670808856121\n",
      "Learning rule:\n",
      "Q[[1, 0],1] <- 1.4080676733658872 + 0.1 * (2.984670808856121 - 1.4080676733658872)\n",
      "Q[[1, 0],1] = 1.5657279869149106\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(DASH_LINE)\n",
    "print('Test bar has capacity')\n",
    "print(DASH_LINE) \n",
    "agent = QAttendance(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "action = 0\n",
    "state = [action, 0]\n",
    "print('Initial state:', state)\n",
    "agent.decisions.append(action)\n",
    "agent.prev_state_ = tuple(state)\n",
    "for i in range(10):\n",
    "\tprint(f'---------- Round {i} ----------')\n",
    "\tpreferences = agent.determine_action_preferences(state)\n",
    "\tprint(f'Action preferences in state {state}: {preferences}')\n",
    "\taction = agent.make_decision()\n",
    "\tprint('Chosen action:', action)\n",
    "\tnew_state = [action, 0]\n",
    "\tprint('State arrived:', new_state)\n",
    "\tpayoff = agent.payoff(action, new_state)\n",
    "\tprint(f'Payoff action {action}: {payoff}')\n",
    "\tagent.update(payoff, new_state)\n",
    "\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../../images/QAttendance')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "data_folder = Path('../../data/QAttendance')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':100,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f0a5939d72498fb86385e43f98a719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each go_discount_factor:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb53ca728aac4cabbe6104c86a1c953f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37099e3e165d4769b78425bb6bad4fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc08d8fad13c4bfd8b6c228a3c1814e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e66628e20d4a0db0cb8aa9773bcf9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1889e7306fbd4b94a6c4630f7b321916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../../images/QAttendance/efficiency_go_discount_factor.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/QAttendance/inequality_go_discount_factor.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/QAttendance/entropy_go_discount_factor.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/QAttendance/conditional_entropy_go_discount_factor.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "n_points = 5\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=QAttendance,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='go_discount_factor',\n",
    "    values=[x for x in np.linspace(0, 1, n_points)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LaTeX_string = Performer.simple_plots(\n",
    "\tagent_class=QAttendance,\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tsimulation_parameters=simulation_parameters,\n",
    "\tmeasures=['efficiency', 'inequality', 'entropy', 'conditional_entropy'],\n",
    "\timage_folder=image_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=QAttendance,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_points = 10\n",
    "exp.run_sweep2(\n",
    "    parameter1='discount_factor',\n",
    "    values1=[x for x in np.logspace(-2, 0, n_points)],\n",
    "    parameter2='go_discount_factor',\n",
    "    values2=[x for x in np.logspace(-2, 0, n_points)],\n",
    "    file=image_folder / 'sweep_discount_vs_go_discount'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.plot_utils import PlotsAndMeasures\n",
    "\n",
    "p = PlotsAndMeasures(exp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plot_scores_sweep2(\n",
    "    parameter1='discount_factor', \n",
    "    parameter2='go_discount_factor',\n",
    "    file=image_folder / Path('sweep_discount_vs_go_discount.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
