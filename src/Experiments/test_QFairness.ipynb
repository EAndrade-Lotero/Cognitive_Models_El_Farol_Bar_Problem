{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.cognitive_model_agents import QFairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DASH_LINE = '-'*60\n",
    "fixed_parameters = {\n",
    "\t\"threshold\":0.5,\n",
    "\t\"num_agents\":2,\n",
    "}\n",
    "free_parameters = {\n",
    "\t\"learning_rate\": 0.1,\n",
    "\t\"discount_factor\": 0,\n",
    "    \"fairness_bias\":0.8,\n",
    "\t\"go_drive\":0,\n",
    "\t\"inverse_temperature\":32\n",
    "}\n",
    "agent = QFairness(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar is full\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 1]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[(1, 1),0] <- 0.0 + 0.1 * (0.0 - 0.0)\n",
      "Q[(1, 1),0] = 0.0\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [0, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go frequency: 0.6666666666666666 --- [1, 0, 1]\n",
      "Go fairness: -0.16666666666666663\n",
      "Reward: -1\n",
      "Estimated long term reward: -1.0\n",
      "Reward with go fairness: -0.33333333333333326\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- 0.0 + 0.1 * (-0.33333333333333326 - 0.0)\n",
      "Q[[0, 1],1] = -0.033333333333333326\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 0, 1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.1 * (0.0 - 0.0)\n",
      "Q[[1, 1],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 1]: [ 0.         -0.03333333]\n",
      "Action probabilities:\n",
      "no go:0.7439624913247579 ---- go:0.256037508675242\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go frequency: 0.6 --- [1, 0, 1, 0, 1]\n",
      "Go fairness: -0.09999999999999998\n",
      "Reward: -1\n",
      "Estimated long term reward: -1.0\n",
      "Reward with go fairness: -0.2799999999999999\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.033333333333333326 + 0.1 * (-0.2799999999999999 - -0.033333333333333326)\n",
      "Q[[0, 1],1] = -0.05799999999999998\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 1]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 0, 1, 0, 1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 1],0] <- 0.0 + 0.1 * (0.0 - 0.0)\n",
      "Q[[1, 1],0] = 0.0\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 1]: [ 0.    -0.058]\n",
      "Action probabilities:\n",
      "no go:0.8648300338622283 ---- go:0.13516996613777166\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.42857142857142855 --- [1, 0, 1, 0, 1, 0, 0]\n",
      "Go fairness: -0.07142857142857145\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: -0.05714285714285716\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- 0.0 + 0.1 * (-0.05714285714285716 - 0.0)\n",
      "Q[[0, 1],0] = -0.005714285714285717\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [0, 1]: [-0.00571429 -0.058     ]\n",
      "Action probabilities:\n",
      "no go:0.8419943959011562 ---- go:0.15800560409884382\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.375 --- [1, 0, 1, 0, 1, 0, 0, 0]\n",
      "Go fairness: -0.125\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: -0.1\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.005714285714285717 + 0.1 * (-0.1 - -0.005714285714285717)\n",
      "Q[[0, 1],0] = -0.015142857142857147\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 1]: [-0.01514286 -0.058     ]\n",
      "Action probabilities:\n",
      "no go:0.7976108627238324 ---- go:0.2023891372761676\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.3333333333333333 --- [1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "Go fairness: -0.16666666666666669\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: -0.13333333333333336\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.015142857142857147 + 0.1 * (-0.13333333333333336 - -0.015142857142857147)\n",
      "Q[[0, 1],0] = -0.02696190476190477\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [0, 1]: [-0.0269619 -0.058    ]\n",
      "Action probabilities:\n",
      "no go:0.7297232754404226 ---- go:0.27027672455957746\n",
      "Chosen action: 0\n",
      "State arrived: [0, 1]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.3 --- [1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "Go fairness: -0.2\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: -0.16000000000000003\n",
      "Learning rule:\n",
      "Q[[0, 1],0] <- -0.02696190476190477 + 0.1 * (-0.16000000000000003 - -0.02696190476190477)\n",
      "Q[[0, 1],0] = -0.040265714285714294\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 1]: [-0.04026571 -0.058     ]\n",
      "Action probabilities:\n",
      "no go:0.6381854535813217 ---- go:0.3618145464186782\n",
      "Chosen action: 1\n",
      "State arrived: [1, 1]\n",
      "Payoff action 1: -1\n",
      "Average go frequency: 0.36363636363636365 --- [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n",
      "Go fairness: 0.13636363636363635\n",
      "Reward: -1\n",
      "Estimated long term reward: -1.0\n",
      "Reward with go fairness: -0.09090909090909087\n",
      "Learning rule:\n",
      "Q[[0, 1],1] <- -0.05799999999999998 + 0.1 * (-0.09090909090909087 - -0.05799999999999998)\n",
      "Q[[0, 1],1] = -0.06129090909090907\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(DASH_LINE)\n",
    "print('Test bar is full')\n",
    "print(DASH_LINE)    \n",
    "action = 1\n",
    "state = [action, 1]\n",
    "print('Initial state:', state)\n",
    "agent.decisions.append(action)\n",
    "agent.prev_state_ = tuple(state)\n",
    "for i in range(10):\n",
    "\tprint(f'---------- Round {i} ----------')\n",
    "\tpreferences = agent.determine_action_preferences(state)\n",
    "\tprint(f'Action preferences in state {state}: {preferences}')\n",
    "\taction = agent.make_decision()\n",
    "\tprint('Chosen action:', action)\n",
    "\tnew_state = [action, 1]\n",
    "\tprint('State arrived:', new_state)\n",
    "\tpayoff = agent.payoff(action, new_state)\n",
    "\tprint(f'Payoff action {action}: {payoff}')\n",
    "\tagent.update(payoff, new_state)\n",
    "\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "Test bar has capacity\n",
      "------------------------------------------------------------\n",
      "Initial state: [1, 0]\n",
      "---------- Round 0 ----------\n",
      "Action preferences in state [1, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go frequency: 1.0 --- [1, 1]\n",
      "Go fairness: -0.5\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.0\n",
      "Reward with go fairness: -0.20000000000000007\n",
      "Learning rule:\n",
      "Q[(1, 0),1] <- 0.0 + 0.1 * (-0.20000000000000007 - 0.0)\n",
      "Q[(1, 0),1] = -0.020000000000000007\n",
      "---------- Round 1 ----------\n",
      "Action preferences in state [1, 0]: [ 0.   -0.02]\n",
      "Action probabilities:\n",
      "no go:0.6547534606063193 ---- go:0.3452465393936807\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.6666666666666666 --- [1, 1, 0]\n",
      "Go fairness: 0.16666666666666663\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.1333333333333333\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.0 + 0.1 * (0.1333333333333333 - 0.0)\n",
      "Q[[1, 0],0] = 0.01333333333333333\n",
      "---------- Round 2 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 1, 0, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.1 * (0.0 - 0.0)\n",
      "Q[[0, 0],0] = 0.0\n",
      "---------- Round 3 ----------\n",
      "Action preferences in state [0, 0]: [0. 0.]\n",
      "Action probabilities:\n",
      "no go:0.5 ---- go:0.5\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go frequency: 0.6 --- [1, 1, 0, 0, 1]\n",
      "Go fairness: -0.09999999999999998\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.0\n",
      "Reward with go fairness: 0.11999999999999997\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.0 + 0.1 * (0.11999999999999997 - 0.0)\n",
      "Q[[0, 0],1] = 0.011999999999999997\n",
      "---------- Round 4 ----------\n",
      "Action preferences in state [1, 0]: [ 0.01333333 -0.02      ]\n",
      "Action probabilities:\n",
      "no go:0.7439624913247581 ---- go:0.256037508675242\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 1, 0, 0, 1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.01333333333333333 + 0.1 * (0.0 - 0.01333333333333333)\n",
      "Q[[1, 0],0] = 0.011999999999999997\n",
      "---------- Round 5 ----------\n",
      "Action preferences in state [0, 0]: [0.    0.012]\n",
      "Action probabilities:\n",
      "no go:0.40516250910988494 ---- go:0.594837490890115\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go frequency: 0.5714285714285714 --- [1, 1, 0, 0, 1, 0, 1]\n",
      "Go fairness: -0.0714285714285714\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.0\n",
      "Reward with go fairness: 0.14285714285714285\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.011999999999999997 + 0.1 * (0.14285714285714285 - 0.011999999999999997)\n",
      "Q[[0, 0],1] = 0.02508571428571428\n",
      "---------- Round 6 ----------\n",
      "Action preferences in state [1, 0]: [ 0.012 -0.02 ]\n",
      "Action probabilities:\n",
      "no go:0.7357510183102304 ---- go:0.2642489816897696\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 1, 0, 0, 1, 0, 1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.011999999999999997 + 0.1 * (0.0 - 0.011999999999999997)\n",
      "Q[[1, 0],0] = 0.010799999999999997\n",
      "---------- Round 7 ----------\n",
      "Action preferences in state [0, 0]: [0.         0.02508571]\n",
      "Action probabilities:\n",
      "no go:0.3094391010674128 ---- go:0.6905608989325872\n",
      "Chosen action: 1\n",
      "State arrived: [1, 0]\n",
      "Payoff action 1: 1\n",
      "Average go frequency: 0.5555555555555556 --- [1, 1, 0, 0, 1, 0, 1, 0, 1]\n",
      "Go fairness: -0.05555555555555558\n",
      "Reward: 1\n",
      "Estimated long term reward: 1.0\n",
      "Reward with go fairness: 0.1555555555555555\n",
      "Learning rule:\n",
      "Q[[0, 0],1] <- 0.02508571428571428 + 0.1 * (0.1555555555555555 - 0.02508571428571428)\n",
      "Q[[0, 0],1] = 0.0381326984126984\n",
      "---------- Round 8 ----------\n",
      "Action preferences in state [1, 0]: [ 0.0108 -0.02  ]\n",
      "Action probabilities:\n",
      "no go:0.728217964461852 ---- go:0.2717820355381479\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.5 --- [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
      "Go fairness: -0.0\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: 0.0\n",
      "Learning rule:\n",
      "Q[[1, 0],0] <- 0.010799999999999997 + 0.1 * (0.0 - 0.010799999999999997)\n",
      "Q[[1, 0],0] = 0.009719999999999998\n",
      "---------- Round 9 ----------\n",
      "Action preferences in state [0, 0]: [0.        0.0381327]\n",
      "Action probabilities:\n",
      "no go:0.22789310059446655 ---- go:0.7721068994055335\n",
      "Chosen action: 0\n",
      "State arrived: [0, 0]\n",
      "Payoff action 0: 0\n",
      "Average go frequency: 0.45454545454545453 --- [1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n",
      "Go fairness: -0.04545454545454547\n",
      "Reward: 0\n",
      "Estimated long term reward: 0.0\n",
      "Reward with go fairness: -0.036363636363636376\n",
      "Learning rule:\n",
      "Q[[0, 0],0] <- 0.0 + 0.1 * (-0.036363636363636376 - 0.0)\n",
      "Q[[0, 0],0] = -0.0036363636363636377\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(DASH_LINE)\n",
    "print('Test bar has capacity')\n",
    "print(DASH_LINE) \n",
    "agent = QFairness(\n",
    "\tfixed_parameters=fixed_parameters,\n",
    "\tfree_parameters=free_parameters,\n",
    "\tn=0\n",
    ")\n",
    "agent.debug = True\n",
    "action = 1\n",
    "state = [action, 0]\n",
    "print('Initial state:', state)\n",
    "agent.decisions.append(action)\n",
    "agent.prev_state_ = tuple(state)\n",
    "for i in range(10):\n",
    "\tprint(f'---------- Round {i} ----------')\n",
    "\tpreferences = agent.determine_action_preferences(state)\n",
    "\tprint(f'Action preferences in state {state}: {preferences}')\n",
    "\taction = agent.make_decision()\n",
    "\tprint('Chosen action:', action)\n",
    "\tnew_state = [action, 0]\n",
    "\tprint('State arrived:', new_state)\n",
    "\tpayoff = agent.payoff(action, new_state)\n",
    "\tprint(f'Payoff action {action}: {payoff}')\n",
    "\tagent.update(payoff, new_state)\n",
    "\tstate = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "image_folder = Path('../../images/QFairness')\n",
    "image_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "simulation_parameters = {\n",
    "\t'num_episodes':100,\n",
    "\t'num_rounds':100,\n",
    "\t'verbose':False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Utils.interaction import Experiment\n",
    "\n",
    "exp = Experiment(\n",
    "    agent_class=QFairness,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67219f24d36d4a15a8d8c2fe789b9937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running models for each fairness_bias:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2196231b3514baf83bcefe04277ea09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9234c716460a40dd845e5ed80f3c5556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6bea445a6642d289b96674e43ac078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d692b0320eb14105a0db2bb13c4e1842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ae33ac66174b7da04bb88e3101417e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting efficiency...\n",
      "Plot saved to ../../images/QFairness/efficiency_fairness_bias.pdf\n",
      "Plotting inequality...\n",
      "Plot saved to ../../images/QFairness/inequality_fairness_bias.pdf\n",
      "Plotting entropy...\n",
      "Plot saved to ../../images/QFairness/entropy_fairness_bias.pdf\n",
      "Plotting conditional_entropy...\n",
      "Plot saved to ../../images/QFairness/conditional_entropy_fairness_bias.pdf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Utils.interaction import Performer\n",
    "\n",
    "n_points = 5\n",
    "LaTeX_string = Performer.sweep(\n",
    "    agent_class=QFairness,\n",
    "    fixed_parameters=fixed_parameters,\n",
    "    free_parameters=free_parameters,\n",
    "    simulation_parameters=simulation_parameters,\n",
    "    sweep_parameter='fairness_bias',\n",
    "    values=[x for x in np.linspace(0, 1, n_points)],\n",
    "    image_folder=image_folder,\n",
    "    measures=['efficiency', 'inequality', 'entropy', 'conditional_entropy']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_repositorios",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
